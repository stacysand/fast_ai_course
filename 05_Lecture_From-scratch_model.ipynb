{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc03dab0-bf5c-4403-9e32-b7d2abefb93e",
   "metadata": {},
   "source": [
    "# 05_Lecture_From-scratch_model\n",
    "\n",
    "Linear Model >> Matrix multiplication >> NN >> DL on Titanic Kaggel database\n",
    "\n",
    "This Lecture is progressively biult on the following Notebooks:\n",
    "1) https://www.kaggle.com/code/jhoward/linear-model-and-neural-net-from-scratch\n",
    "2) https://www.kaggle.com/code/jhoward/why-you-should-use-a-framework\n",
    "\n",
    "**Comments:**\n",
    "\n",
    "Demonstration of how important a feature engineering is: Titanic using Name only [0.81818] (https://www.kaggle.com/code/cdeotte/titanic-using-name-only-0-81818/notebook) and Titanic - Advanced Feature Engineering Tutorial (https://www.kaggle.com/code/gunesevitan/titanic-advanced-feature-engineering-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ea302a-33ca-4b9b-8351-c5f6fed5ef98",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7dcee53c-8b2a-495e-a245-6af3fc1d98b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import tensor\n",
    "from fastai.data.transforms import RandomSplitter\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c275b511-c4c6-47c4-a9d5-436a4fc67d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a little bit correct the formats in which tabular data will be printed\n",
    "np.set_printoptions(linewidth=140)\n",
    "torch.set_printoptions(linewidth=140, sci_mode=False, edgeitems=7)\n",
    "pd.set_option('display.width', 140)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efb6afd-7607-458d-9862-3c5a559117a1",
   "metadata": {},
   "source": [
    "# Step 1. Prepare the dataset\n",
    "## 1) Download the dataset\n",
    "The dataset was downloaded on PC from the web https://www.kaggle.com/competitions/titanic/data# and saved to the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82b5b82f-c3e5-4ccd-9f1e-8fecb9874baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Thayer)</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass                                                 Name     Sex   Age  SibSp  Parch            Ticket  \\\n",
       "0              1         0       3                              Braund, Mr. Owen Harris    male  22.0      1      0         A/5 21171   \n",
       "1              2         1       1  Cumings, Mrs. John Bradley (Florence Briggs Thayer)  female  38.0      1      0          PC 17599   \n",
       "2              3         1       3                               Heikkinen, Miss. Laina  female  26.0      0      0  STON/O2. 3101282   \n",
       "3              4         1       1         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1      0            113803   \n",
       "4              5         0       3                             Allen, Mr. William Henry    male  35.0      0      0            373450   \n",
       "..           ...       ...     ...                                                  ...     ...   ...    ...    ...               ...   \n",
       "886          887         0       2                                Montvila, Rev. Juozas    male  27.0      0      0            211536   \n",
       "887          888         1       1                         Graham, Miss. Margaret Edith  female  19.0      0      0            112053   \n",
       "888          889         0       3             Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1      2        W./C. 6607   \n",
       "889          890         1       1                                Behr, Mr. Karl Howell    male  26.0      0      0            111369   \n",
       "890          891         0       3                                  Dooley, Mr. Patrick    male  32.0      0      0            370376   \n",
       "\n",
       "        Fare Cabin Embarked  \n",
       "0     7.2500   NaN        S  \n",
       "1    71.2833   C85        C  \n",
       "2     7.9250   NaN        S  \n",
       "3    53.1000  C123        S  \n",
       "4     8.0500   NaN        S  \n",
       "..       ...   ...      ...  \n",
       "886  13.0000   NaN        S  \n",
       "887  30.0000   B42        S  \n",
       "888  23.4500   NaN        S  \n",
       "889  30.0000  C148        C  \n",
       "890   7.7500   NaN        Q  \n",
       "\n",
       "[891 rows x 12 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('/Users/hela/Code/fast_ai/titanic/train.csv')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62650afe-706d-471b-9349-789cc62b6371",
   "metadata": {},
   "source": [
    "Here's how we get a quick summary of all the non-numeric columns in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0412fca-9abe-49b3-8ebc-7c3f2e812146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>204</td>\n",
       "      <td>889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>891</td>\n",
       "      <td>2</td>\n",
       "      <td>681</td>\n",
       "      <td>147</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>347082</td>\n",
       "      <td>B96 B98</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>577</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Name   Sex  Ticket    Cabin Embarked\n",
       "count                       891   891     891      204      889\n",
       "unique                      891     2     681      147        3\n",
       "top     Braund, Mr. Owen Harris  male  347082  B96 B98        S\n",
       "freq                          1   577       7        4      644"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe(include=[object])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14cb636-f1b3-4a3e-aa73-e3776b994ec9",
   "metadata": {},
   "source": [
    "## 2) Clean NA\n",
    "For a plain linear model, we multiply each column by some coefficients. But we can see that our dataset contains missing values. We can't multiply something by a missing value! So let's clean NA cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "244cc5df-4210-4126-835c-6ca93e0b81f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# investigate how many NAs are in the table\n",
    "train_df.isna()  # all values\n",
    "train_df.isna().sum()  # summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85bf2d3-a249-4b20-9827-8912a012a8b4",
   "metadata": {},
   "source": [
    "We'll need to replace the missing values with something.  \n",
    "It doesn't generally matter too much what we choose. We'll use the most common value to replace empty cells. For this, we can use the \"mode: function.\n",
    "(One wrinkle is that it returns more than one row in the case of ties, so we just grab the first row with iloc[0])\n",
    "\n",
    "After this, we can use \"fillna\" to replace the missing values with the mode of each column. We'll do it \"in place\" -- meaning that we'll change the dataframe itself, rather than returning a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bea21b69-ccd8-4068-85c7-095b86228ce7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId                      1\n",
      "Survived                       0.0\n",
      "Pclass                         3.0\n",
      "Name           Abbing, Mr. Anthony\n",
      "Sex                           male\n",
      "Age                           24.0\n",
      "SibSp                          0.0\n",
      "Parch                          0.0\n",
      "Ticket                        1601\n",
      "Fare                          8.05\n",
      "Cabin                      B96 B98\n",
      "Embarked                         S\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PassengerId    0\n",
       "Survived       0\n",
       "Pclass         0\n",
       "Name           0\n",
       "Sex            0\n",
       "Age            0\n",
       "SibSp          0\n",
       "Parch          0\n",
       "Ticket         0\n",
       "Fare           0\n",
       "Cabin          0\n",
       "Embarked       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get modes for each column\n",
    "modes = train_df.mode().iloc[0]\n",
    "# look at them\n",
    "print(modes)\n",
    "# replace NAs\n",
    "train_df.fillna(modes, inplace=True)\n",
    "# check that there is no NAs anymore\n",
    "train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0257b7fa-3948-4985-996d-991a0228e58a",
   "metadata": {},
   "source": [
    "## 3) Log transformation for some Vs\n",
    "We can see that Fare contains mainly values of around 0 to 30, but there's a few really big ones. This is very common with fields contain monetary values, and it can cause problems for our model, because once that column is multiplied by a coefficient later, the few rows with really big values will dominate the result.\n",
    "\n",
    "You can see the issue most clearly visually by looking at a histogram, which shows a long tail to the right (and don't forget: if you're not entirely sure what a histogram is, Google \"histogram tutorial\" and do a bit of reading before continuing on):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a052a73-52fd-4e9d-bef4-848e14eef9d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df['Fare'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf1aa68-9a22-4331-b208-a52467e20c4b",
   "metadata": {},
   "source": [
    "Let's perform log data transformation.\n",
    "(however, that there are zeros in the Fare column, and log(0) is infinite -- to fix this, we'll simply add 1 to all values first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40886d85-a4b2-4b77-a06e-d5d18b2efa26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# transform data\n",
    "train_df['LogFare'] = np.log(train_df['Fare']+1)\n",
    "# see the result on histogram\n",
    "train_df['LogFare'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347c51ff-9d6d-4f9d-b0a0-00b4ad80b794",
   "metadata": {},
   "source": [
    "## 4) Dummy variables\n",
    "Clearly we can't multiply strings like male or S by coefficients, so we need to replace those with numbers.\n",
    "\n",
    "We do that by creating new columns containing dummy variables. A dummy variable is a column that contains a 1 where a particular column contains a particular value, or a 0 otherwise. For instance, we could create a dummy variable for Sex='male', which would be a new column containing 1 for rows where Sex is 'male', and 0 for rows where it isn't.\n",
    "\n",
    "Pandas can create these automatically using get_dummies, which also remove the original columns. We'll create dummy variables for Pclass, even although it's numeric, since the numbers 1, 2, and 3 correspond to first, second, and third class cabins - not to counts or measures that make sense to multiply by. We'll also create dummies for Sex and Embarked since we'll want to use those as predictors in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa50b006-c35e-423e-80f2-fa98e02adc84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>LogFare</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>B96 B98</td>\n",
       "      <td>2.110213</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Thayer)</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>4.280593</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>B96 B98</td>\n",
       "      <td>2.188856</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>3.990834</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>B96 B98</td>\n",
       "      <td>2.202765</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>B96 B98</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>3.433987</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>B96 B98</td>\n",
       "      <td>3.196630</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>3.433987</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>B96 B98</td>\n",
       "      <td>2.169054</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived                                                 Name   Age  SibSp  Parch            Ticket     Fare    Cabin  \\\n",
       "0              1         0                              Braund, Mr. Owen Harris  22.0      1      0         A/5 21171   7.2500  B96 B98   \n",
       "1              2         1  Cumings, Mrs. John Bradley (Florence Briggs Thayer)  38.0      1      0          PC 17599  71.2833      C85   \n",
       "2              3         1                               Heikkinen, Miss. Laina  26.0      0      0  STON/O2. 3101282   7.9250  B96 B98   \n",
       "3              4         1         Futrelle, Mrs. Jacques Heath (Lily May Peel)  35.0      1      0            113803  53.1000     C123   \n",
       "4              5         0                             Allen, Mr. William Henry  35.0      0      0            373450   8.0500  B96 B98   \n",
       "..           ...       ...                                                  ...   ...    ...    ...               ...      ...      ...   \n",
       "886          887         0                                Montvila, Rev. Juozas  27.0      0      0            211536  13.0000  B96 B98   \n",
       "887          888         1                         Graham, Miss. Margaret Edith  19.0      0      0            112053  30.0000      B42   \n",
       "888          889         0             Johnston, Miss. Catherine Helen \"Carrie\"  24.0      1      2        W./C. 6607  23.4500  B96 B98   \n",
       "889          890         1                                Behr, Mr. Karl Howell  26.0      0      0            111369  30.0000     C148   \n",
       "890          891         0                                  Dooley, Mr. Patrick  32.0      0      0            370376   7.7500  B96 B98   \n",
       "\n",
       "      LogFare  Sex_female  Sex_male  Pclass_1  Pclass_2  Pclass_3  Embarked_C  Embarked_Q  Embarked_S  \n",
       "0    2.110213           0         1         0         0         1           0           0           1  \n",
       "1    4.280593           1         0         1         0         0           1           0           0  \n",
       "2    2.188856           1         0         0         0         1           0           0           1  \n",
       "3    3.990834           1         0         1         0         0           0           0           1  \n",
       "4    2.202765           0         1         0         0         1           0           0           1  \n",
       "..        ...         ...       ...       ...       ...       ...         ...         ...         ...  \n",
       "886  2.639057           0         1         0         1         0           0           0           1  \n",
       "887  3.433987           1         0         1         0         0           0           0           1  \n",
       "888  3.196630           1         0         0         0         1           0           0           1  \n",
       "889  3.433987           0         1         1         0         0           1           0           0  \n",
       "890  2.169054           0         1         0         0         1           0           1           0  \n",
       "\n",
       "[891 rows x 18 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.get_dummies(train_df, columns=[\"Sex\",\"Pclass\",\"Embarked\"], dtype=int)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a6ad63-bf63-4b99-b8e1-e9df19cb4930",
   "metadata": {},
   "source": [
    "We can see that new columns have been added to the end -- one for each of the possible values of each of the three columns we requested, and that those three requested columns have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8712e179-7e66-41ce-b871-dcd284f00aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sex_male  Sex_female  Pclass_1  Pclass_2  Pclass_3  Embarked_C  Embarked_Q  Embarked_S\n",
       "0         1           0         0         0         1           0           0           1\n",
       "1         0           1         1         0         0           1           0           0\n",
       "2         0           1         0         0         1           0           0           1\n",
       "3         0           1         1         0         0           0           0           1\n",
       "4         1           0         0         0         1           0           0           1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "added_cols = ['Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\n",
    "train_df[added_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb00937-493f-4a78-ba60-527a581c525b",
   "metadata": {},
   "source": [
    "## 5) Data normalizing\n",
    "Actually, here this step will be performed later, see the \"Create predictions\" section.\n",
    "\n",
    "## 6) IVs and DVs\n",
    "Now we can create our independent (predictors) and dependent (target) variables. They both need to be PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb1eccc8-c328-49bc-82db-9dec1921f31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DV:\n",
    "dv = tensor(train_df.Survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "317d8d54-8d20-4bc9-8336-9d22853156d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[22.0000,  1.0000,  0.0000,  2.1102,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n",
       "        [38.0000,  1.0000,  0.0000,  4.2806,  0.0000,  1.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
       "        [26.0000,  0.0000,  0.0000,  2.1889,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n",
       "        [35.0000,  1.0000,  0.0000,  3.9908,  0.0000,  1.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
       "        [35.0000,  0.0000,  0.0000,  2.2028,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n",
       "        [24.0000,  0.0000,  0.0000,  2.2469,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000],\n",
       "        [54.0000,  0.0000,  0.0000,  3.9677,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
       "        ...,\n",
       "        [25.0000,  0.0000,  0.0000,  2.0857,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n",
       "        [39.0000,  0.0000,  5.0000,  3.4054,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000],\n",
       "        [27.0000,  0.0000,  0.0000,  2.6391,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
       "        [19.0000,  0.0000,  0.0000,  3.4340,  0.0000,  1.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
       "        [24.0000,  1.0000,  2.0000,  3.1966,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n",
       "        [26.0000,  0.0000,  0.0000,  3.4340,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
       "        [32.0000,  0.0000,  0.0000,  2.1691,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IVS:\n",
    "iv_columns = ['Age', 'SibSp', 'Parch', 'LogFare'] + added_cols\n",
    "iv = tensor(train_df[iv_columns].values, dtype=torch.float)\n",
    "iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0efefa91-65f1-4811-860d-816a32bd6063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([891, 12])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# number of rows and columns we have for ivs:\n",
    "print(iv.shape)\n",
    "# the rank of the tensor:\n",
    "print(len(iv.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55e735f-dab8-4c9e-96c8-5c72b8a32f0d",
   "metadata": {},
   "source": [
    "# Step 2. Linear model\n",
    "## 1. Setting up\n",
    "Now we can work on calculating our predictions and our loss. In this section, we're going to manually do a single step of calculating predictions and loss for every row of our data. Our first model will be a simple linear model. \n",
    "\n",
    "### 1) Initialize coefficiants (fn)\n",
    "We'll need a coefficient for each column in train_iv.  \n",
    "We'll pick random numbers in the range (-0.5,0.5), and set our manual seed so that my explanations in the prose in this notebook will be consistent with what you see when you run it.  \n",
    "The number of coefficients we need in the number of our IVs, which is 12 in torch.Size([891, 12]).  \n",
    "Since torch will generate numbers between 0 and 1, we will substruct them by 0.5 so they will be centered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2011e01-4ea8-4a02-a56a-95a880fb20e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4629,  0.1386,  0.2409, -0.2262, -0.2632, -0.3147,  0.4876,  0.3136,  0.2799, -0.4392,  0.2103,  0.3625])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(442)  # for getting results reproducable\n",
    "n_coeff = iv.shape[1]  # number of coeffs\n",
    "coeffs = torch.rand(n_coeff)-0.5  # create 12 random numbers and center them\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373dea0d-9dcb-4b01-92df-e0bd70baa3a5",
   "metadata": {},
   "source": [
    "Pop it into (not)final function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a75fa35-9399-4eea-9863-5c73d325efdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1722,  0.2324, -0.3575, -0.0010, -0.1833, -0.2411,  0.0489,  0.0866, -0.0534,  0.3132, -0.1487, -0.2551])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_coeffs():\n",
    "    return torch.rand(n_coeff)-0.5\n",
    "init_coeffs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2837c85c-ba7f-4d97-9d3d-3a5ebed15d78",
   "metadata": {},
   "source": [
    "### 2) Preds (fn)\n",
    "\n",
    "1) Create predictions (multiply IVs by coefficients)\n",
    "   \n",
    "Our predictions will be calculated by multiplying each row by the coefficients, and adding them up. One interesting point here is that we don't need a separate constant term (also known as a \"bias\" or \"intercept\" term), or a column of all 1s to give the same effect has having a constant term. That's because our dummy variables already cover the entire dataset -- e.g. there's a column for \"male\" and a column for \"female\", and everyone in the dataset is in exactly one of these; therefore, we don't need a separate intercept term to cover rows that aren't otherwise part of a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c4b0be7-c9b2-4d05-af82-54431f25b0dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-10.1838,   0.1386,   0.0000,  -0.4772,  -0.2632,  -0.0000,   0.0000,   0.0000,   0.2799,  -0.0000,   0.0000,   0.3625],\n",
       "        [-17.5902,   0.1386,   0.0000,  -0.9681,  -0.0000,  -0.3147,   0.4876,   0.0000,   0.0000,  -0.4392,   0.0000,   0.0000],\n",
       "        [-12.0354,   0.0000,   0.0000,  -0.4950,  -0.0000,  -0.3147,   0.0000,   0.0000,   0.2799,  -0.0000,   0.0000,   0.3625],\n",
       "        [-16.2015,   0.1386,   0.0000,  -0.9025,  -0.0000,  -0.3147,   0.4876,   0.0000,   0.0000,  -0.0000,   0.0000,   0.3625],\n",
       "        [-16.2015,   0.0000,   0.0000,  -0.4982,  -0.2632,  -0.0000,   0.0000,   0.0000,   0.2799,  -0.0000,   0.0000,   0.3625],\n",
       "        [-11.1096,   0.0000,   0.0000,  -0.5081,  -0.2632,  -0.0000,   0.0000,   0.0000,   0.2799,  -0.0000,   0.2103,   0.0000],\n",
       "        [-24.9966,   0.0000,   0.0000,  -0.8973,  -0.2632,  -0.0000,   0.4876,   0.0000,   0.0000,  -0.0000,   0.0000,   0.3625],\n",
       "        ...,\n",
       "        [-11.5725,   0.0000,   0.0000,  -0.4717,  -0.2632,  -0.0000,   0.0000,   0.0000,   0.2799,  -0.0000,   0.0000,   0.3625],\n",
       "        [-18.0531,   0.0000,   1.2045,  -0.7701,  -0.0000,  -0.3147,   0.0000,   0.0000,   0.2799,  -0.0000,   0.2103,   0.0000],\n",
       "        [-12.4983,   0.0000,   0.0000,  -0.5968,  -0.2632,  -0.0000,   0.0000,   0.3136,   0.0000,  -0.0000,   0.0000,   0.3625],\n",
       "        [ -8.7951,   0.0000,   0.0000,  -0.7766,  -0.0000,  -0.3147,   0.4876,   0.0000,   0.0000,  -0.0000,   0.0000,   0.3625],\n",
       "        [-11.1096,   0.1386,   0.4818,  -0.7229,  -0.0000,  -0.3147,   0.0000,   0.0000,   0.2799,  -0.0000,   0.0000,   0.3625],\n",
       "        [-12.0354,   0.0000,   0.0000,  -0.7766,  -0.2632,  -0.0000,   0.4876,   0.0000,   0.0000,  -0.4392,   0.0000,   0.0000],\n",
       "        [-14.8128,   0.0000,   0.0000,  -0.4905,  -0.2632,  -0.0000,   0.0000,   0.0000,   0.2799,  -0.0000,   0.2103,   0.0000]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# element pairwise multiplication, broadcastning (aka looping 12*12 elements mult, 891 times)\n",
    "iv*coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38c3105-18d1-4938-aa79-60756b437a05",
   "metadata": {},
   "source": [
    "2) Data normalizing (Back to data preparation step)\n",
    "\n",
    "We can see we've got a problem here. The sums of each row will be dominated by the first column, which is Age, since that's bigger on average than all the others.\n",
    "\n",
    "Let's make all the columns contain numbers from 0 to 1, by dividing each column by its max().\n",
    "(dim=0 means that we want a max for each column, not rows)\n",
    "\n",
    "That is dividing a matrix by a vector -- what on earth does that mean?!? The trick here is that we're taking advantage of a technique in numpy and PyTorch (and many other languages, going all the way back to APL) called broadcasting. In short, this acts as if there's a separate copy of the vector for every row of the matrix, so it divides each row of the matrix by the vector. In practice, it doesn't actually make any copies, and does the whole thing in a highly optimized way, taking full advantage of modern CPUs (or, indeed, GPUs, if we're using them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0353c325-3ab7-4b9e-8a37-c40ab6a52712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2750, 0.1250, 0.0000, 0.3381, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n",
       "        [0.4750, 0.1250, 0.0000, 0.6859, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
       "        [0.3250, 0.0000, 0.0000, 0.3507, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n",
       "        [0.4375, 0.1250, 0.0000, 0.6395, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
       "        [0.4375, 0.0000, 0.0000, 0.3530, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n",
       "        [0.3000, 0.0000, 0.0000, 0.3600, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000],\n",
       "        [0.6750, 0.0000, 0.0000, 0.6358, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
       "        ...,\n",
       "        [0.3125, 0.0000, 0.0000, 0.3342, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n",
       "        [0.4875, 0.0000, 0.8333, 0.5456, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000],\n",
       "        [0.3375, 0.0000, 0.0000, 0.4229, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
       "        [0.2375, 0.0000, 0.0000, 0.5502, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
       "        [0.3000, 0.1250, 0.3333, 0.5122, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n",
       "        [0.3250, 0.0000, 0.0000, 0.5502, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
       "        [0.4000, 0.0000, 0.0000, 0.3476, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals,indices = iv.max(dim=0)  # returns 2 things by default: the actual value and a row index. we nee donly vals\n",
    "iv = iv / vals\n",
    "iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd8b62c8-dbcb-4d10-8497-0c37fe553662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1273,  0.0173,  0.0000, -0.0765, -0.2632, -0.0000,  0.0000,  0.0000,  0.2799, -0.0000,  0.0000,  0.3625],\n",
       "        [-0.2199,  0.0173,  0.0000, -0.1551, -0.0000, -0.3147,  0.4876,  0.0000,  0.0000, -0.4392,  0.0000,  0.0000],\n",
       "        [-0.1504,  0.0000,  0.0000, -0.0793, -0.0000, -0.3147,  0.0000,  0.0000,  0.2799, -0.0000,  0.0000,  0.3625],\n",
       "        [-0.2025,  0.0173,  0.0000, -0.1446, -0.0000, -0.3147,  0.4876,  0.0000,  0.0000, -0.0000,  0.0000,  0.3625],\n",
       "        [-0.2025,  0.0000,  0.0000, -0.0798, -0.2632, -0.0000,  0.0000,  0.0000,  0.2799, -0.0000,  0.0000,  0.3625],\n",
       "        [-0.1389,  0.0000,  0.0000, -0.0814, -0.2632, -0.0000,  0.0000,  0.0000,  0.2799, -0.0000,  0.2103,  0.0000],\n",
       "        [-0.3125,  0.0000,  0.0000, -0.1438, -0.2632, -0.0000,  0.4876,  0.0000,  0.0000, -0.0000,  0.0000,  0.3625],\n",
       "        ...,\n",
       "        [-0.1447,  0.0000,  0.0000, -0.0756, -0.2632, -0.0000,  0.0000,  0.0000,  0.2799, -0.0000,  0.0000,  0.3625],\n",
       "        [-0.2257,  0.0000,  0.2008, -0.1234, -0.0000, -0.3147,  0.0000,  0.0000,  0.2799, -0.0000,  0.2103,  0.0000],\n",
       "        [-0.1562,  0.0000,  0.0000, -0.0956, -0.2632, -0.0000,  0.0000,  0.3136,  0.0000, -0.0000,  0.0000,  0.3625],\n",
       "        [-0.1099,  0.0000,  0.0000, -0.1244, -0.0000, -0.3147,  0.4876,  0.0000,  0.0000, -0.0000,  0.0000,  0.3625],\n",
       "        [-0.1389,  0.0173,  0.0803, -0.1158, -0.0000, -0.3147,  0.0000,  0.0000,  0.2799, -0.0000,  0.0000,  0.3625],\n",
       "        [-0.1504,  0.0000,  0.0000, -0.1244, -0.2632, -0.0000,  0.4876,  0.0000,  0.0000, -0.4392,  0.0000,  0.0000],\n",
       "        [-0.1852,  0.0000,  0.0000, -0.0786, -0.2632, -0.0000,  0.0000,  0.0000,  0.2799, -0.0000,  0.2103,  0.0000]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repeat multiplication:\n",
    "iv*coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a08aeb4-be98-42a7-8393-863e0aec5f58",
   "metadata": {},
   "source": [
    "3) Create predictions (second attempt)\n",
    "\n",
    "We can now create predictions from our linear model, by adding up the rows of the product.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16f466ab-0929-40fb-8263-3af924f17328",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([     0.1927,     -0.6239,      0.0979,      0.2056,      0.0968,      0.0066,      0.1306,      0.3476,      0.1613,     -0.6285,\n",
       "             0.2579,      0.0796,      0.1836,      0.2457,      0.1676,     -0.0595,      0.2014,      0.1783,      0.0589,     -0.6892,\n",
       "             0.0909,      0.1205,      0.0089,      0.2945,      0.2614,      0.1999,     -0.6378,      0.4071,     -0.0425,      0.1611,\n",
       "            -0.5679,     -0.5688,     -0.0420,     -0.0576,     -0.5197,      0.2173,     -0.6378,      0.1778,      0.1515,     -0.6284,\n",
       "             0.0284,      0.1104,     -0.6406,     -0.4960,     -0.0136,      0.1605,      0.0038,     -0.0420,     -0.6399,      0.1345,\n",
       "             0.3138,      0.1788,     -0.6902,      0.0915,     -0.7009,      0.3176,      0.1514,     -0.6638,      0.3084,      0.3422,\n",
       "            -0.6262,      0.1563,      0.1831,      0.3664,     -0.4753,     -0.6049,      0.1051,      0.1890,      0.2996,      0.1812,\n",
       "             0.1392,      0.2618,      0.1351,     -0.6549,      0.0472,      0.1563,      0.1611,      0.1605,      0.3651,      0.0598,\n",
       "             0.1684,      0.1261,     -0.0422,      0.2845,      0.1745,      0.0864,      0.2951,      0.1605,      0.3325,      0.1605,\n",
       "             0.1315,      0.1844,      0.1884,      0.2150,     -0.0387,      0.1605,     -0.7551,     -0.4586,      0.0896,      0.1140,\n",
       "             0.0865,      0.1611,      0.3475,      0.1060,      0.1204,      0.1379,      0.1280,      0.1616,      0.0801,     -0.0630,\n",
       "             0.1710,     -0.6398,      0.1720,      0.1430,     -0.6716,      0.1783,     -0.2596,      0.1503,     -0.5134,      0.3398,\n",
       "             0.1698,      0.1605,     -0.6841,      0.0777,      0.1566,     -0.5654,      0.0095,      0.1643,     -0.6696,      0.0435,\n",
       "            -0.6927,      0.1878,     -0.0263,      0.0915,      0.1725,     -0.6225,      0.3860,      0.2455,      0.2024,     -0.5126,\n",
       "            -0.6336,      0.1218,      0.1038,      0.0428,      0.2172,      0.2288,      0.1441,      0.2613,      0.1625,      0.0742,\n",
       "             0.0234,      0.2727,     -0.0218,      0.1258,      0.1635,     -0.6195,      0.0043,      0.1257,      0.1581,      0.3049,\n",
       "             0.0618,      0.0278,      0.1500,      0.1986,      0.3485,      0.2962,      0.2908,      0.1233,      0.3287,      0.0703,\n",
       "             0.1056,      0.1898,      0.2889,      0.1783,     -0.6641,      0.2534,      0.2137,     -0.6785,      0.1436,      0.1708,\n",
       "             0.2535,     -0.6283,      0.3506,      0.3482,      0.2712,      0.3055,     -0.0477,      0.2063,     -0.0486,      0.0916,\n",
       "             0.0806,      0.2073,      0.1560,      0.3335,     -0.6425,     -0.7828,      0.0095,      0.0951,     -0.0420,      0.1269,\n",
       "             0.1319,      0.3049,      0.1094,     -0.7622,      0.1952,      0.2679,      0.1090,     -0.6811,      0.0043,     -0.5718,\n",
       "             0.1647,      0.0468,      0.1754,      0.1436,      0.0268,     -0.6000,      0.0921,      0.0664,     -0.6090,      0.1507,\n",
       "             0.2067,      0.1610,      0.0042,      0.1611,      0.2209,      0.1672,      0.2144,      0.1841,      0.2130,      0.1622,\n",
       "             0.1894,      0.1326,     -0.0255,      0.3223,      0.1855,      0.1110,      0.0561,      0.2756,      0.2144,      0.1282,\n",
       "            -0.6948,     -0.0477,      0.1565,      0.1759,     -0.6725,      0.0512,      0.1043,      0.2035,      0.2860,     -0.0017,\n",
       "             0.1638,      0.1290,      0.1080,      0.1200,      0.0601,     -0.6625,     -0.5640,      0.1998,     -0.6949,     -0.0072,\n",
       "             0.0095,      0.3854,      0.1844,      0.3554,     -0.0420,      0.1160,      0.2617,      0.1731,      0.0573,      0.1547,\n",
       "             0.3224,      0.2345,      0.0548,     -0.5128,     -0.0420,      0.0299,     -0.0113,      0.2740,      0.1725,      0.0719,\n",
       "            -0.2278,      0.1381,      0.2014,      0.1894,      0.3286,     -0.6957,      0.1204,      0.1727,      0.0742,     -0.0304,\n",
       "             0.2262,     -0.5228,     -0.6925,      0.1059,      0.1611,     -0.4753,     -0.6349,      0.4393,      0.3230,     -0.7153,\n",
       "            -0.0420,      0.0072,      0.2692,     -0.0236,      0.1605,      0.4970,     -0.5761,     -0.5176,     -0.6617,     -0.5869,\n",
       "            -0.5658,     -0.4574,      0.1490,      0.1379,      0.1017,      0.0982,      0.1204,      0.0022,      0.2511,     -0.6181,\n",
       "             0.1754,      0.1437,     -0.0584,      0.1683,      0.3049,     -0.6528,     -0.0455,      0.0574,      0.0946,     -0.4664,\n",
       "            -0.0443,      0.2010,      0.2245,      0.2145,      0.2362,      0.1611,      0.2837,     -0.6814,      0.0389,      0.1961,\n",
       "             0.3393,      0.3267,      0.1552,      0.1725,      0.1089,      0.1269,      0.0343,      0.1033,      0.3168,      0.0539,\n",
       "             0.1618,      0.3181,     -0.5282,      0.1455,     -0.6378,      0.1319,      0.3024,      0.0459,     -0.0425,     -0.0425,\n",
       "             0.2037,     -0.6610,     -0.7934,      0.1011,      0.0038,      0.1291,     -0.7532,     -0.6892,     -0.0420,     -0.5592,\n",
       "            -0.4883,      0.2193,      0.1894,     -0.5203,      0.2903,     -0.5480,      0.1239,     -0.4849,     -0.5967,      0.1905,\n",
       "            -0.7061,     -0.5016,      0.1147,      0.2063,      0.1611,      0.1525,      0.4001,      0.0574,      0.0095,     -0.6316,\n",
       "             0.3024,      0.1788,      0.1725,     -0.5537,      0.1650,      0.1731,      0.0693,      0.0272,      0.1912,      0.1046,\n",
       "             0.0742,      0.1489,      0.1372,      0.1321,      0.1298,      0.1214,      0.0054,      0.3448,      0.1789,      0.1622,\n",
       "             0.1611,      0.0133,      0.0461,      0.2740,      0.0452,      0.1090,      0.0949,      0.2419,      0.1436,      0.2333,\n",
       "            -0.6406,      0.0269,      0.1322,      0.1241,      0.2218,      0.1638,      0.0972,      0.1320,      0.0095,      0.1142,\n",
       "             0.3047,      0.1033,      0.0162,      0.2049,      0.1684,      0.3782,      0.1919,      0.2695,      0.1924,      0.1450,\n",
       "             0.0387,      0.1782,      0.1731,      0.1037,      0.1602,      0.4840,      0.2168,      0.2700,     -0.5372,      0.1610,\n",
       "             0.1804,      0.1473,     -0.5101,     -0.6441,      0.1605,     -0.6695,      0.0906,      0.2701,     -0.0164,      0.0095,\n",
       "             0.1890,      0.1026,      0.1817,      0.0395,      0.1605,      0.0837,      0.2740,      0.1427,      0.0096,     -0.5126,\n",
       "             0.1638,      0.0771,      0.1463,     -0.6710,      0.1140,      0.3041,      0.1214,      0.1531,      0.1742,      0.2625,\n",
       "             0.3538,      0.2740,      0.0100,     -0.1224,     -0.5060,      0.1622,      0.1867,     -0.6745,      0.1257,      0.2821,\n",
       "             0.1473,      0.1812,      0.1436,     -0.7677,      0.1778,     -0.6606,     -0.7199,      0.1396,      0.3062,      0.1615,\n",
       "             0.1986,     -0.0247,     -0.0415,      0.0281,      0.2808,     -0.4719,      0.1313,      0.3278,      0.1027,      0.0819,\n",
       "            -0.0195,      0.1605,      0.2588,     -0.7100,      0.1627,      0.1861,      0.0761,     -0.0288,      0.0510,      0.1148,\n",
       "             0.1970,      0.1727,     -0.6378,     -0.6285,     -0.6378,     -0.0860,     -0.0164,      0.2521,      0.0742,      0.2630,\n",
       "             0.2878,     -0.6378,     -0.5398,     -0.6467,      0.0719,      0.2814,      0.2063,     -0.6093,      0.1410,     -0.4554,\n",
       "             0.2524,      0.2993,      0.2877,      0.1256,     -0.6563,      0.0971,      0.1493,     -0.6255,      0.1345,      0.2924,\n",
       "            -0.4038,      0.1372,      0.0091,     -0.6262,      0.1217,      0.1080,     -0.6609,     -0.5505,      0.2081,      0.0312,\n",
       "             0.0095,      0.0685,      0.1539,      0.1605,      0.1090,      0.1581,      0.1900,      0.2083,     -0.6378,      0.1150,\n",
       "            -0.0344,      0.1199,      0.2586,     -0.0420,      0.2067,      0.1699,      0.0690,      0.1806,     -0.6948,      0.1147,\n",
       "             0.1497,     -0.6054,     -0.0191,     -0.5578,     -0.6438,      0.3525,      0.0404,     -0.6634,      0.1720,      0.1605,\n",
       "             0.1007,     -0.7083,      0.0307,      0.0383,      0.0966,      0.1115,      0.0947,      0.0956,     -0.6378,     -0.6281,\n",
       "             0.1765,      0.1611,      0.3114,      0.0447,     -0.5375,      0.0865,      0.1264,      0.3056,     -0.6059,      0.1213,\n",
       "             0.1942,      0.1647,     -0.0477,      0.0095,      0.0968,      0.1683,      0.1408,      0.0917,      0.2793,      0.1739,\n",
       "            -0.6606,      0.2169,     -0.5829,      0.1786,      0.1548,      0.1069,     -0.1631,      0.2556,      0.1495,      0.0095,\n",
       "            -0.0005,      0.0085,     -0.5250,      0.4480,      0.2860,      0.1037,      0.1147,      0.1712,      0.1569,      0.1547,\n",
       "             0.1844,     -0.5592,      0.3265,      0.0935,     -0.5126,     -0.6330,      0.1900,     -0.6692,      0.1625,      0.1168,\n",
       "             0.1611,      0.1822,      0.1763,     -0.0423,     -0.0029,      0.1524,      0.1611,     -0.0538,      0.1841,     -0.6418,\n",
       "             0.1545,     -0.7303,      0.1960,      0.0933,      0.2014,      0.1061,      0.1725,      0.1616,      0.0505,      0.2700,\n",
       "             0.0537,      0.2810,     -0.0807,      0.1378,      0.2740,      0.1963,      0.1576,      0.1372,      0.1969,     -0.6091,\n",
       "            -0.0436,     -0.5288,      0.1792,      0.3248,     -0.0105,     -0.5718,      0.2733,      0.1818,      0.1962,      0.2946,\n",
       "             0.2777,     -0.5537,      0.0935,     -0.6435,      0.1195,      0.0150,      0.0447,     -0.0419,     -0.6118,      0.0579,\n",
       "            -0.5499,      0.2645,     -0.6372,      0.0037,      0.1670,      0.0677,      0.0041,      0.2240,      0.2259,     -0.6049,\n",
       "            -0.5473,      0.3278,      0.1826,      0.1262,      0.0163,      0.1910,     -0.6830,      0.1166,     -0.0135,      0.1095,\n",
       "             0.2390,      0.2225,      0.1205,      0.0279,      0.3034,      0.1812,      0.1277,     -0.0420,      0.1661,      0.1210,\n",
       "             0.1734,     -0.5943,      0.2740,      0.1841,      0.1841,      0.1114,      0.0585,     -0.6435,      0.1611,      0.1611,\n",
       "             0.3236,      0.2372,     -0.4748,      0.1547,      0.1205,      0.0843,      0.2333,      0.0921,      0.3496,     -0.0311,\n",
       "             0.2805,      0.2903,      0.1030,      0.1669,      0.0294,      0.3671,      0.1383,      0.2172,      0.1026,      0.1824,\n",
       "             0.1410,      0.0660,     -0.6146,      0.2509,      0.2079,      0.0993,     -0.4879,     -0.0796,     -0.0115,      0.1129,\n",
       "             0.1551,      0.0224,     -0.0570,     -0.6378,      0.0715,      0.1964,      0.0095,      0.2045,      0.0095,      0.1326,\n",
       "            -0.6256,      0.3072,      0.2946,      0.2221,      0.1589,      0.1580,      0.1460,      0.1667,      0.3597,     -0.6398,\n",
       "             0.0095,      0.2008,      0.2535,     -0.4789,      0.1553,      0.0915,      0.1325,      0.0660,     -0.6725,      0.0947,\n",
       "             0.1205,      0.1197,      0.4471,     -0.4664,      0.1477,      0.1211,      0.3612,      0.1448,      0.0915,      0.2172,\n",
       "             0.1495,      0.0366,      0.1218,      0.3167,      0.1228,      0.4480,      0.1153,     -0.6426,      0.0576,      0.3317,\n",
       "             0.1272,      0.1407,      0.3670,      0.1174,      0.3427,      0.0129,      0.0935,     -0.4461,      0.0095,      0.0174,\n",
       "            -0.6427,      0.3574,     -0.6378,      0.1670,      0.1942,     -0.5951,      0.1754,      0.1605,      0.0472,     -0.4778,\n",
       "             0.1841,      0.2318,     -0.5654,     -0.6949,      0.1986,      0.0584,      0.3049,     -0.7042,      0.1632,     -0.5509,\n",
       "             0.3797,     -0.1277,     -0.5696,      0.3489,      0.0047,      0.1790,      0.1473,      0.1716,     -0.6014,     -0.6378,\n",
       "             0.0782,      0.2171,      0.1383,      0.2535,      0.1783,      0.0227,     -0.6770,      0.2647,      0.1551,      0.3230,\n",
       "             0.1495,      0.1767,      0.3310,      0.0238,     -0.7016,     -0.6371,      0.1770,      0.1900,      0.1611,     -0.7108,\n",
       "             0.1374,      0.1090,      0.1118,      0.1623,      0.1589,      0.0271,      0.1610,      0.3010,      0.1706,     -0.4897,\n",
       "            -0.0368])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = (iv*coeffs).sum(axis=1)  # over the columns\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b3f077-03c0-462b-869b-b2c81cc2b3f6",
   "metadata": {},
   "source": [
    "4. Pop it into final function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fe9b878-7854-4be2-8d5d-7ecf83d8fe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_preds(coeffs, iv): \n",
    "    return (iv*coeffs).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0769ad9-f28e-49a5-882e-1cd908b2eb86",
   "metadata": {},
   "source": [
    "### 3) Loss (fn)\n",
    "\n",
    "Of course, these predictions aren't going to be any use, since our coefficients are random -- they're just a starting point for our gradient descent process.\n",
    "\n",
    "To do gradient descent, we need a loss function. Taking the average error of the rows (i.e. the absolute value of the difference between the prediction and the dependent) is generally a reasonable approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3926594-e618-4c84-9be7-13e329981e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5382)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.abs(preds-dv).mean()  # mean absolute error cost function\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f23879-c542-4fc5-9fc8-d7b3c67e98b3",
   "metadata": {},
   "source": [
    "Pop it into final function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5be5ef8-9955-41b7-8d41-b3299d83f75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(coeffs, iv, dv): \n",
    "    return torch.abs(calc_preds(coeffs, iv)-dv).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cac672-7f81-4e25-81d0-7277ec2c2b43",
   "metadata": {},
   "source": [
    "## 2. Gradient Descent\n",
    "In this section, we're going to do a single \"epoch\" of gradient descent manually. \n",
    "\n",
    "### 1) Init coeff with grads (fn)\n",
    "The only thing we're going to automate is calculating gradients, because let's face it that's pretty tedious and entirely pointless to do by hand! To get PyTorch to calculate gradients, we'll need to call requires_grad_() on our coeffs \n",
    "(_ a the end of requirew_grad is inplace operation, so its gonna change coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46eb0a74-cf7a-4e73-a315-9137a3003df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4629,  0.1386,  0.2409, -0.2262, -0.2632, -0.3147,  0.4876,  0.3136,  0.2799, -0.4392,  0.2103,  0.3625], requires_grad=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeffs.requires_grad_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26895729-a72d-40f0-85eb-233f8f97770a",
   "metadata": {},
   "source": [
    "### 2) Update coeffs (fn)\n",
    "Use backward() to ask PyTorch to calculate gradients now.\n",
    "\n",
    "Note that each time we call backward, the gradients are actually added to whatever is in the .grad attribute. So if you try running the steps again, you will see that our .grad values are have doubled. That's because it added the gradients a second time. For this reason, after we use the gradients to do a gradient descent step, we need to set them back to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "059835e9-cc1c-4180-ad09-3a4796ca60a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5382, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = calc_loss(coeffs, iv, dv)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060503f6-d7f1-4bc5-b6d9-4963936041d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96e4b2ea-4f78-49c0-9ee7-79d769a8bfb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0106,  0.0129, -0.0041, -0.0484,  0.2099, -0.2132, -0.1212, -0.0247,  0.1425, -0.1886, -0.0191,  0.2043])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.backward()\n",
    "coeffs.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60ac58c2-8b04-4ba5-b7e6-19f71c1a06df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0212,  0.0258, -0.0082, -0.0969,  0.4198, -0.4265, -0.2424, -0.0494,  0.2851, -0.3771, -0.0382,  0.4085])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = calc_loss(coeffs, iv, dv)\n",
    "loss.backward()\n",
    "coeffs.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09144e44-893a-4af0-8415-323ce4f14d60",
   "metadata": {},
   "source": [
    "### 3) GD step (fn)\n",
    "We can now do one gradient descent step, and check that our loss decreases.\n",
    "\n",
    "First we again calculate initial loss, then we should see that each time we run the code, our loss (the result of the cell) should become less and less:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4576aa72-e33d-4915-8a59-a1e9222efea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4945)\n"
     ]
    }
   ],
   "source": [
    "loss = calc_loss(coeffs, iv, dv)\n",
    "loss.backward()\n",
    "\n",
    "with torch.no_grad():\n",
    "    coeffs.sub_(coeffs.grad * 0.1)  # substract from the coeffs their gradients * learning rate\n",
    "    coeffs.grad.zero_()\n",
    "    print(calc_loss(coeffs, iv, dv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2fc6ec-9729-4f2b-ac04-b0bdd25e0aa4",
   "metadata": {},
   "source": [
    "## 3. Training\n",
    "### 1) Validation set\n",
    "Use RandomSplitter to get indices that will split our data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3186363f-ebb0-41d8-acae-cadf50a8376d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#178) [np.int64(303),np.int64(778),np.int64(531),np.int64(385),np.int64(134),np.int64(476),np.int64(691),np.int64(443),np.int64(386),np.int64(128),np.int64(579),np.int64(65),np.int64(869),np.int64(359),np.int64(202),np.int64(187),np.int64(456),np.int64(880),np.int64(705),np.int64(797)...]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_split, valid_split = RandomSplitter(seed=42)(train_df)\n",
    "valid_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184a28b3-2092-4c58-a205-fe12d9de0a63",
   "metadata": {},
   "source": [
    "Apply those indicies to our independent and dependent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21f792f3-9f43-4ff4-8a9e-760ee42debd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(713, 178)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iv, valid_iv = iv[train_split], iv[valid_split]\n",
    "train_dv, valid_dv = dv[train_split], dv[valid_split]\n",
    "len(train_iv),len(valid_iv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c748be24-2622-45d0-9eb1-f698b6e8b033",
   "metadata": {},
   "source": [
    "### 2) Define functions from above (fns)\n",
    "   \n",
    "Create functions for the three things we did manually above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6691dc94-709b-43d6-9994-5a86a897344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize coeffs\n",
    "def init_coeffs():\n",
    "    return (torch.rand(n_coeff)-0.5).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f5ca858-9dea-4ee9-951e-f43dc1dc3236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GD step\n",
    "def GD_step(coeffs, lr):\n",
    "    loss = calc_loss(coeffs, train_iv, train_dv)\n",
    "    loss.backward()\n",
    "    with torch.no_grad(): update_coeffs(coeffs, lr)\n",
    "    print(f\"{loss:.3f}\", end=\"; \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6775846-bcf0-4c8d-afe3-f2eb05d1379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating coeffs\n",
    "def update_coeffs(coeffs, lr):\n",
    "    coeffs.sub_(coeffs.grad * lr)\n",
    "    coeffs.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0d8e43-f7f3-4851-8084-bed1060ee95e",
   "metadata": {},
   "source": [
    "### 3) Combine fns into train_model fn (fn)\n",
    "Put them all together to train the model fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f33bebd7-9c34-481c-bc81-22186c1b356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs=30, lr=0.01):\n",
    "    torch.manual_seed(442)\n",
    "    coeffs = init_coeffs()\n",
    "    for i in range(epochs):\n",
    "        GD_step(coeffs, lr=lr)\n",
    "    return coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fce7be3-7f01-4bc1-9810-8a7dbbf05897",
   "metadata": {},
   "source": [
    "### 4) Train the model\n",
    "Try our fn. Our loss will print at the end of every step, so we hope we'll see it going down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "04448af2-bf97-452c-9049-ddc7c7ae8f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.536; 0.502; 0.477; 0.454; 0.431; 0.409; 0.388; 0.367; 0.349; 0.336; 0.330; 0.326; 0.329; 0.304; 0.314; 0.296; 0.300; 0.289; "
     ]
    }
   ],
   "source": [
    "coeffs = train_model(18, lr=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "30c7c75a-6c8c-4bd4-a4b4-c2ad4b821473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Age': tensor(-0.2694),\n",
       " 'SibSp': tensor(0.0901),\n",
       " 'Parch': tensor(0.2359),\n",
       " 'LogFare': tensor(0.0280),\n",
       " 'Sex_male': tensor(-0.3990),\n",
       " 'Sex_female': tensor(0.2345),\n",
       " 'Pclass_1': tensor(0.7232),\n",
       " 'Pclass_2': tensor(0.4112),\n",
       " 'Pclass_3': tensor(0.3601),\n",
       " 'Embarked_C': tensor(0.0955),\n",
       " 'Embarked_Q': tensor(0.2395),\n",
       " 'Embarked_S': tensor(0.2122)}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the coeffs for each column:\n",
    "def show_coeffs(): \n",
    "    return dict(zip(iv_columns, coeffs.requires_grad_(False)))\n",
    "show_coeffs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dea7806-27cb-423e-885d-dde52b9e3468",
   "metadata": {},
   "source": [
    "## 4. Results & accuracy (fn)\n",
    "The Kaggle competition is not, however, scored by absolute error (which is our loss function). It's scored by accuracy -- the proportion of rows where we correctly predict survival. Let's see how accurate we were on the validation set. First, calculate the predictions on updated coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c8d7e09-2432-4a1e-a28f-1bf81bb6e2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = calc_preds(coeffs,valid_iv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8b4826-a539-4260-b80b-c3433293344f",
   "metadata": {},
   "source": [
    "We'll assume that any passenger with a score of over 0.5 is predicted to survive. So that means we're correct for each row where preds>0.5 is the same as the dependent variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f0002a7-fa0f-4d0b-a934-210af8591b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False, False, False,  True,  True, False])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = valid_dv.bool()==(preds>0.5)\n",
    "results[:16]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997ab6b5-aa4c-490e-8bb7-172a3c1f3c7f",
   "metadata": {},
   "source": [
    "Average accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "01c2770c-a2c9-4acc-889a-683531ad7ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7865)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84c193c-bbc0-472e-8970-fb8d0744d509",
   "metadata": {},
   "source": [
    "Pop it into final function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ed64c8c-06ad-408f-9a6e-24ab2b6f614f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7865)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def acc(coeffs):\n",
    "    return (valid_dv.bool()==(calc_preds(coeffs, valid_iv)>0.5)).float().mean()\n",
    "acc(coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e13e582-215d-4e9d-b11e-a68f36d691d8",
   "metadata": {},
   "source": [
    "## 5. Sigmoid (fn)\n",
    "Looking at our predictions, there's one obvious problem -- some of our predictions of the probability of survival are >1, and some are <0. To fix this, we should pass every prediction through the sigmoid function, which has a minimum at zero and maximum at one. PyTorch already defines that function for us, so we can modify calc_preds to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7a53264c-27e8-4765-ad2b-cc2c989db625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_preds(coeffs, iv): \n",
    "    return torch.sigmoid((iv*coeffs).sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a891359a-7b8b-4988-9143-a4f34fb62cdb",
   "metadata": {},
   "source": [
    "Train new model with sogmoid wil get us better result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "305a8059-389d-4848-b552-91f03535b09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.510; 0.327; 0.294; 0.207; 0.201; 0.199; 0.198; 0.197; 0.196; 0.196; 0.196; 0.195; 0.195; 0.195; 0.195; 0.195; 0.195; 0.195; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; "
     ]
    }
   ],
   "source": [
    "coeffs = train_model(lr=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9f1265a1-08cb-4b61-9bf8-e2f8dbc0fc76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8258)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc(coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2e653e33-bf20-4ca8-88f4-a7d37030d607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Age': tensor(-1.5061),\n",
       " 'SibSp': tensor(-1.1575),\n",
       " 'Parch': tensor(-0.4267),\n",
       " 'LogFare': tensor(0.2543),\n",
       " 'Sex_male': tensor(-10.3320),\n",
       " 'Sex_female': tensor(8.4185),\n",
       " 'Pclass_1': tensor(3.8389),\n",
       " 'Pclass_2': tensor(2.1398),\n",
       " 'Pclass_3': tensor(-6.2331),\n",
       " 'Embarked_C': tensor(1.4771),\n",
       " 'Embarked_Q': tensor(2.1168),\n",
       " 'Embarked_S': tensor(-4.7958)}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_coeffs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5437ba-266d-4447-92f8-22cf9e1661a2",
   "metadata": {},
   "source": [
    "## 6. Final submission\n",
    "### 1) Prepare the test dataset\n",
    "First, prepare the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8bfd86e9-93ad-4081-aee4-ee94c1a85756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>3</td>\n",
       "      <td>Spector, Mr. Woolf</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A.5. 3236</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "      <td>Oliva y Ocana, Dona. Fermina</td>\n",
       "      <td>female</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17758</td>\n",
       "      <td>108.9000</td>\n",
       "      <td>C105</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>3</td>\n",
       "      <td>Saether, Mr. Simon Sivertsen</td>\n",
       "      <td>male</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SOTON/O.Q. 3101262</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>3</td>\n",
       "      <td>Ware, Mr. Frederick</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>359309</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>3</td>\n",
       "      <td>Peter, Master. Michael J</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2668</td>\n",
       "      <td>22.3583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Pclass                                          Name     Sex   Age  SibSp  Parch              Ticket      Fare Cabin  \\\n",
       "0            892       3                              Kelly, Mr. James    male  34.5      0      0              330911    7.8292   NaN   \n",
       "1            893       3              Wilkes, Mrs. James (Ellen Needs)  female  47.0      1      0              363272    7.0000   NaN   \n",
       "2            894       2                     Myles, Mr. Thomas Francis    male  62.0      0      0              240276    9.6875   NaN   \n",
       "3            895       3                              Wirz, Mr. Albert    male  27.0      0      0              315154    8.6625   NaN   \n",
       "4            896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female  22.0      1      1             3101298   12.2875   NaN   \n",
       "..           ...     ...                                           ...     ...   ...    ...    ...                 ...       ...   ...   \n",
       "413         1305       3                            Spector, Mr. Woolf    male   NaN      0      0           A.5. 3236    8.0500   NaN   \n",
       "414         1306       1                  Oliva y Ocana, Dona. Fermina  female  39.0      0      0            PC 17758  108.9000  C105   \n",
       "415         1307       3                  Saether, Mr. Simon Sivertsen    male  38.5      0      0  SOTON/O.Q. 3101262    7.2500   NaN   \n",
       "416         1308       3                           Ware, Mr. Frederick    male   NaN      0      0              359309    8.0500   NaN   \n",
       "417         1309       3                      Peter, Master. Michael J    male   NaN      1      1                2668   22.3583   NaN   \n",
       "\n",
       "    Embarked  \n",
       "0          Q  \n",
       "1          S  \n",
       "2          Q  \n",
       "3          S  \n",
       "4          S  \n",
       "..       ...  \n",
       "413        S  \n",
       "414        C  \n",
       "415        S  \n",
       "416        S  \n",
       "417        C  \n",
       "\n",
       "[418 rows x 11 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dowmload test dataset\n",
    "test_df = pd.read_csv('/Users/hela/Code/fast_ai/titanic/test.csv')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3b21ab80-cb6b-4124-a121-29c51efcffaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId    0\n",
       "Survived       0\n",
       "Name           0\n",
       "Age            0\n",
       "SibSp          0\n",
       "Parch          0\n",
       "Ticket         0\n",
       "Fare           0\n",
       "Cabin          0\n",
       "LogFare        0\n",
       "Sex_female     0\n",
       "Sex_male       0\n",
       "Pclass_1       0\n",
       "Pclass_2       0\n",
       "Pclass_3       0\n",
       "Embarked_C     0\n",
       "Embarked_Q     0\n",
       "Embarked_S     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean NA (get modes for each column and replace NAs)\n",
    "modes_test = test_df.mode().iloc[0]\n",
    "test_df.fillna(modes_test, inplace=True)\n",
    "train_df.isna().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5559cff9-758c-4b66-8b8a-f8b2fb00b621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lag transformation for some Vs\n",
    "test_df['LogFare'] = np.log(test_df['Fare']+1)\n",
    "test_df['LogFare'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d1fa8c4b-6bc1-4929-8dc0-75f4cb3a173d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>LogFare</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>B57 B59 B63 B66</td>\n",
       "      <td>2.178064</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>B57 B59 B63 B66</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>B57 B59 B63 B66</td>\n",
       "      <td>2.369075</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>B57 B59 B63 B66</td>\n",
       "      <td>2.268252</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>B57 B59 B63 B66</td>\n",
       "      <td>2.586824</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>Spector, Mr. Woolf</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A.5. 3236</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>B57 B59 B63 B66</td>\n",
       "      <td>2.202765</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>Oliva y Ocana, Dona. Fermina</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17758</td>\n",
       "      <td>108.9000</td>\n",
       "      <td>C105</td>\n",
       "      <td>4.699571</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>Saether, Mr. Simon Sivertsen</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SOTON/O.Q. 3101262</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>B57 B59 B63 B66</td>\n",
       "      <td>2.110213</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>Ware, Mr. Frederick</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>359309</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>B57 B59 B63 B66</td>\n",
       "      <td>2.202765</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>Peter, Master. Michael J</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2668</td>\n",
       "      <td>22.3583</td>\n",
       "      <td>B57 B59 B63 B66</td>\n",
       "      <td>3.150952</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId                                          Name   Age  SibSp  Parch              Ticket      Fare            Cabin  \\\n",
       "0            892                              Kelly, Mr. James  34.5      0      0              330911    7.8292  B57 B59 B63 B66   \n",
       "1            893              Wilkes, Mrs. James (Ellen Needs)  47.0      1      0              363272    7.0000  B57 B59 B63 B66   \n",
       "2            894                     Myles, Mr. Thomas Francis  62.0      0      0              240276    9.6875  B57 B59 B63 B66   \n",
       "3            895                              Wirz, Mr. Albert  27.0      0      0              315154    8.6625  B57 B59 B63 B66   \n",
       "4            896  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  22.0      1      1             3101298   12.2875  B57 B59 B63 B66   \n",
       "..           ...                                           ...   ...    ...    ...                 ...       ...              ...   \n",
       "413         1305                            Spector, Mr. Woolf  21.0      0      0           A.5. 3236    8.0500  B57 B59 B63 B66   \n",
       "414         1306                  Oliva y Ocana, Dona. Fermina  39.0      0      0            PC 17758  108.9000             C105   \n",
       "415         1307                  Saether, Mr. Simon Sivertsen  38.5      0      0  SOTON/O.Q. 3101262    7.2500  B57 B59 B63 B66   \n",
       "416         1308                           Ware, Mr. Frederick  21.0      0      0              359309    8.0500  B57 B59 B63 B66   \n",
       "417         1309                      Peter, Master. Michael J  21.0      1      1                2668   22.3583  B57 B59 B63 B66   \n",
       "\n",
       "      LogFare  Sex_female  Sex_male  Pclass_1  Pclass_2  Pclass_3  Embarked_C  Embarked_Q  Embarked_S  \n",
       "0    2.178064           0         1         0         0         1           0           1           0  \n",
       "1    2.079442           1         0         0         0         1           0           0           1  \n",
       "2    2.369075           0         1         0         1         0           0           1           0  \n",
       "3    2.268252           0         1         0         0         1           0           0           1  \n",
       "4    2.586824           1         0         0         0         1           0           0           1  \n",
       "..        ...         ...       ...       ...       ...       ...         ...         ...         ...  \n",
       "413  2.202765           0         1         0         0         1           0           0           1  \n",
       "414  4.699571           1         0         1         0         0           1           0           0  \n",
       "415  2.110213           0         1         0         0         1           0           0           1  \n",
       "416  2.202765           0         1         0         0         1           0           0           1  \n",
       "417  3.150952           0         1         0         0         1           1           0           0  \n",
       "\n",
       "[418 rows x 17 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dummy variables\n",
    "test_df = pd.get_dummies(test_df, columns=[\"Sex\",\"Pclass\",\"Embarked\"], dtype=int)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "193fa94d-1845-4753-b9cc-3e0a2505e574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([418, 12])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[34.5000,  0.0000,  0.0000,  2.1781,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000],\n",
       "        [47.0000,  1.0000,  0.0000,  2.0794,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n",
       "        [62.0000,  0.0000,  0.0000,  2.3691,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000],\n",
       "        [27.0000,  0.0000,  0.0000,  2.2683,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n",
       "        [22.0000,  1.0000,  1.0000,  2.5868,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n",
       "        [14.0000,  0.0000,  0.0000,  2.3248,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n",
       "        [30.0000,  0.0000,  0.0000,  2.1552,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000],\n",
       "        ...,\n",
       "        [37.0000,  1.0000,  0.0000,  4.5109,  0.0000,  1.0000,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000],\n",
       "        [28.0000,  0.0000,  0.0000,  2.1719,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n",
       "        [21.0000,  0.0000,  0.0000,  2.2028,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n",
       "        [39.0000,  0.0000,  0.0000,  4.6996,  0.0000,  1.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
       "        [38.5000,  0.0000,  0.0000,  2.1102,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n",
       "        [21.0000,  0.0000,  0.0000,  2.2028,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n",
       "        [21.0000,  1.0000,  1.0000,  3.1510,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  1.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IVs:\n",
    "test_iv = tensor(test_df[iv_columns].values, dtype=torch.float)  # iv_columns was created earlier\n",
    "print(test_iv.shape)\n",
    "test_iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1c12b664-9b7c-4974-b07c-15c68e33abc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4539, 0.0000, 0.0000, 0.3490, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000],\n",
       "        [0.6184, 0.1250, 0.0000, 0.3332, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n",
       "        [0.8158, 0.0000, 0.0000, 0.3796, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000],\n",
       "        [0.3553, 0.0000, 0.0000, 0.3634, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n",
       "        [0.2895, 0.1250, 0.1111, 0.4145, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n",
       "        [0.1842, 0.0000, 0.0000, 0.3725, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n",
       "        [0.3947, 0.0000, 0.0000, 0.3453, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.4868, 0.1250, 0.0000, 0.7228, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000],\n",
       "        [0.3684, 0.0000, 0.0000, 0.3480, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n",
       "        [0.2763, 0.0000, 0.0000, 0.3530, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n",
       "        [0.5132, 0.0000, 0.0000, 0.7530, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
       "        [0.5066, 0.0000, 0.0000, 0.3381, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n",
       "        [0.2763, 0.0000, 0.0000, 0.3530, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n",
       "        [0.2763, 0.1250, 0.1111, 0.5049, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalizing\n",
    "vals_test,indices_test = test_iv.max(dim=0)  # returns 2 things by default: the actual value and a row index. we nee donly vals\n",
    "test_iv = test_iv / vals_test\n",
    "test_iv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77958450-eed4-425d-acc1-5d16551b0ded",
   "metadata": {},
   "source": [
    "### 2) Predictions\n",
    "Calculate predictions of which passengers survived in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e5dd9fc4-00d1-4bab-a990-3a6dac96e63e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>LogFare</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>B57 B59 B63 B66</td>\n",
       "      <td>2.178064</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>B57 B59 B63 B66</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>B57 B59 B63 B66</td>\n",
       "      <td>2.369075</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>B57 B59 B63 B66</td>\n",
       "      <td>2.268252</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>B57 B59 B63 B66</td>\n",
       "      <td>2.586824</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>Spector, Mr. Woolf</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A.5. 3236</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>B57 B59 B63 B66</td>\n",
       "      <td>2.202765</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>Oliva y Ocana, Dona. Fermina</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17758</td>\n",
       "      <td>108.9000</td>\n",
       "      <td>C105</td>\n",
       "      <td>4.699571</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>Saether, Mr. Simon Sivertsen</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SOTON/O.Q. 3101262</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>B57 B59 B63 B66</td>\n",
       "      <td>2.110213</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>Ware, Mr. Frederick</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>359309</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>B57 B59 B63 B66</td>\n",
       "      <td>2.202765</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>Peter, Master. Michael J</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2668</td>\n",
       "      <td>22.3583</td>\n",
       "      <td>B57 B59 B63 B66</td>\n",
       "      <td>3.150952</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId                                          Name   Age  SibSp  Parch              Ticket      Fare            Cabin  \\\n",
       "0            892                              Kelly, Mr. James  34.5      0      0              330911    7.8292  B57 B59 B63 B66   \n",
       "1            893              Wilkes, Mrs. James (Ellen Needs)  47.0      1      0              363272    7.0000  B57 B59 B63 B66   \n",
       "2            894                     Myles, Mr. Thomas Francis  62.0      0      0              240276    9.6875  B57 B59 B63 B66   \n",
       "3            895                              Wirz, Mr. Albert  27.0      0      0              315154    8.6625  B57 B59 B63 B66   \n",
       "4            896  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  22.0      1      1             3101298   12.2875  B57 B59 B63 B66   \n",
       "..           ...                                           ...   ...    ...    ...                 ...       ...              ...   \n",
       "413         1305                            Spector, Mr. Woolf  21.0      0      0           A.5. 3236    8.0500  B57 B59 B63 B66   \n",
       "414         1306                  Oliva y Ocana, Dona. Fermina  39.0      0      0            PC 17758  108.9000             C105   \n",
       "415         1307                  Saether, Mr. Simon Sivertsen  38.5      0      0  SOTON/O.Q. 3101262    7.2500  B57 B59 B63 B66   \n",
       "416         1308                           Ware, Mr. Frederick  21.0      0      0              359309    8.0500  B57 B59 B63 B66   \n",
       "417         1309                      Peter, Master. Michael J  21.0      1      1                2668   22.3583  B57 B59 B63 B66   \n",
       "\n",
       "      LogFare  Sex_female  Sex_male  Pclass_1  Pclass_2  Pclass_3  Embarked_C  Embarked_Q  Embarked_S  Survived  \n",
       "0    2.178064           0         1         0         0         1           0           1           0         0  \n",
       "1    2.079442           1         0         0         0         1           0           0           1         0  \n",
       "2    2.369075           0         1         0         1         0           0           1           0         0  \n",
       "3    2.268252           0         1         0         0         1           0           0           1         0  \n",
       "4    2.586824           1         0         0         0         1           0           0           1         0  \n",
       "..        ...         ...       ...       ...       ...       ...         ...         ...         ...       ...  \n",
       "413  2.202765           0         1         0         0         1           0           0           1         0  \n",
       "414  4.699571           1         0         1         0         0           1           0           0         1  \n",
       "415  2.110213           0         1         0         0         1           0           0           1         0  \n",
       "416  2.202765           0         1         0         0         1           0           0           1         0  \n",
       "417  3.150952           0         1         0         0         1           1           0           0         0  \n",
       "\n",
       "[418 rows x 18 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = ((calc_preds(coeffs,test_iv))>0.5).int()\n",
    "test_preds\n",
    "# create a column in our test_data\n",
    "test_df[\"Survived\"] = test_preds\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb98174-9c9d-4c59-ae92-9770aaa1b133",
   "metadata": {},
   "source": [
    "### 3) Final submission\n",
    "The sample submission on the Kaggle competition site shows that we're expected to upload a CSV with just PassengerId and Survived, so let's create that and save it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4e5760ad-8553-475e-b4b7-3ae3ffca5180",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived\n",
       "0            892         0\n",
       "1            893         0\n",
       "2            894         0\n",
       "3            895         0\n",
       "4            896         0\n",
       "..           ...       ...\n",
       "413         1305         0\n",
       "414         1306         1\n",
       "415         1307         0\n",
       "416         1308         0\n",
       "417         1309         0\n",
       "\n",
       "[418 rows x 2 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a submission file\n",
    "subm_df = test_df[[\"PassengerId\",\"Survived\"]]\n",
    "subm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f3e2a755-e36a-4c80-961f-117286351c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId,Survived\n",
      "892,0\n",
      "893,0\n",
      "894,0\n",
      "895,0\n",
      "896,0\n",
      "897,0\n",
      "898,1\n",
      "899,0\n",
      "900,1\n"
     ]
    }
   ],
   "source": [
    "# save the submission file\n",
    "subm_df.to_csv('subm.csv', index=False)\n",
    "!head subm.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63a0e8d-0886-479a-affc-672f100f45b3",
   "metadata": {},
   "source": [
    "# Step 3. Matrix multiplication\n",
    "## 1) Calc preds (matrix mult) (fn)\n",
    "Multiplying elements together and then adding across rows is identical to doing a matrix-vector product! Python uses the @ operator to indicate matrix products, and is supported by PyTorch tensors. Therefore, we can replicate the above calculate more simply like so. It also turns out that this is much faster, because matrix products in PyTorch are very highly optimised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4839ef79-fc55-4f80-bcec-0aefc9df045b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 12.3288, -14.8119, -15.4540, -13.1513, -13.3511, -13.6468,   3.6248,   5.3429, -22.0878,   3.1233, -21.8742, -15.6421, -21.5504,\n",
       "          3.9393, -21.9190, -12.0010, -12.3775,   5.3550, -13.5880,  -3.1015, -21.7237, -12.2081,  12.9767,   4.7427, -21.6525, -14.9135,\n",
       "         -2.7433, -12.3210, -21.5886,   3.9387,   5.3890,  -3.6196, -21.6296, -21.8454,  12.2159,  -3.2275, -12.0289,  13.4560, -21.7230,\n",
       "         -3.1366, -13.2462, -21.7230, -13.6831,  13.3092, -21.6477,  -3.5868, -21.6854, -21.8316, -14.8158,  -2.9386,  -5.3103, -22.2384,\n",
       "        -22.1097, -21.7466, -13.3780, -13.4909, -14.8119, -22.0690, -21.6666, -21.7818,  -5.4439, -21.7407, -12.6551, -21.6671,   4.9238,\n",
       "        -11.5777, -13.3323, -21.9638, -15.3030,   5.0243, -21.7614,   3.1820, -13.4721, -21.7170, -11.6066, -21.5737, -21.7230, -11.9652,\n",
       "        -13.2382, -13.7599, -13.2170,  13.1347, -21.7049, -21.7268,   4.9207,  -7.3198,  -5.3081,   7.1066,  11.4948, -13.3135, -21.8723,\n",
       "        -21.7230,  13.3603, -15.5670,   3.4105,  -7.2857, -13.7197,   3.6909,   3.9763, -14.7227, -21.8268,   3.9387, -21.8743, -21.8367,\n",
       "        -11.8518, -13.6712, -21.8299,   4.9440,  -5.4471, -21.9666,   5.1333,  -3.2187, -11.6008,  13.7920, -21.7230,  12.6369,  -3.7268,\n",
       "        -14.8119, -22.0637,  12.9469, -22.1610,  -6.1827, -14.8119,  -3.2838, -15.4540, -11.6950,  -2.9926,  -3.0110, -21.5664, -13.8268,\n",
       "          7.3426, -21.8418,   5.0744,   5.2582,  13.3415, -21.6289, -13.9898, -21.8112,  -7.3316,   5.2296, -13.4453,  12.7891, -22.1235,\n",
       "        -14.9625,  -3.4339,   6.3089, -21.9839,   3.1968,   7.2400,   2.8558,  -3.1187,   3.7965,   5.4667, -15.1101, -15.0597, -22.9391,\n",
       "        -21.7230,  -3.0346, -13.5206, -21.7011,  13.4425,  -7.2690, -21.8335, -12.0582,  13.0489,   6.7993,   5.2160,   5.0794, -12.6957,\n",
       "        -12.1838,  -3.0873, -21.6070,   7.0745, -21.7170, -22.1001,   6.8159, -11.6002, -21.6310])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# old:\n",
    "(valid_iv*coeffs).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "640b56ac-a466-48f9-9fb2-5e51310914fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 12.3288, -14.8119, -15.4540, -13.1513, -13.3511, -13.6468,   3.6248,   5.3429, -22.0878,   3.1233, -21.8742, -15.6421, -21.5504,\n",
       "          3.9393, -21.9190, -12.0010, -12.3775,   5.3550, -13.5880,  -3.1015, -21.7237, -12.2081,  12.9767,   4.7427, -21.6525, -14.9135,\n",
       "         -2.7433, -12.3210, -21.5886,   3.9387,   5.3890,  -3.6196, -21.6296, -21.8454,  12.2159,  -3.2275, -12.0289,  13.4560, -21.7230,\n",
       "         -3.1366, -13.2462, -21.7230, -13.6831,  13.3092, -21.6477,  -3.5868, -21.6854, -21.8316, -14.8158,  -2.9386,  -5.3103, -22.2384,\n",
       "        -22.1097, -21.7466, -13.3780, -13.4909, -14.8119, -22.0690, -21.6666, -21.7818,  -5.4439, -21.7407, -12.6551, -21.6671,   4.9238,\n",
       "        -11.5777, -13.3323, -21.9638, -15.3030,   5.0243, -21.7614,   3.1820, -13.4721, -21.7170, -11.6066, -21.5737, -21.7230, -11.9652,\n",
       "        -13.2382, -13.7599, -13.2170,  13.1347, -21.7049, -21.7268,   4.9207,  -7.3198,  -5.3081,   7.1066,  11.4948, -13.3135, -21.8723,\n",
       "        -21.7230,  13.3603, -15.5670,   3.4105,  -7.2857, -13.7197,   3.6909,   3.9763, -14.7227, -21.8268,   3.9387, -21.8743, -21.8367,\n",
       "        -11.8518, -13.6712, -21.8299,   4.9440,  -5.4471, -21.9666,   5.1333,  -3.2187, -11.6008,  13.7920, -21.7230,  12.6369,  -3.7268,\n",
       "        -14.8119, -22.0637,  12.9469, -22.1610,  -6.1827, -14.8119,  -3.2838, -15.4540, -11.6950,  -2.9926,  -3.0110, -21.5664, -13.8268,\n",
       "          7.3426, -21.8418,   5.0744,   5.2582,  13.3415, -21.6289, -13.9898, -21.8112,  -7.3316,   5.2296, -13.4453,  12.7891, -22.1235,\n",
       "        -14.9625,  -3.4339,   6.3089, -21.9839,   3.1968,   7.2400,   2.8558,  -3.1187,   3.7965,   5.4667, -15.1101, -15.0597, -22.9391,\n",
       "        -21.7230,  -3.0346, -13.5206, -21.7011,  13.4425,  -7.2690, -21.8335, -12.0582,  13.0489,   6.7993,   5.2160,   5.0794, -12.6957,\n",
       "        -12.1838,  -3.0873, -21.6070,   7.0745, -21.7170, -22.1001,   6.8159, -11.6002, -21.6310])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new:\n",
    "valid_iv@coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe58c35-c954-4d6c-a6d6-3c1cbb1c3820",
   "metadata": {},
   "source": [
    "Let's use this to replace how calc_preds works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f5604aac-c1f0-4f8d-a8dd-8af71be39335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_preds(coeffs, iv): \n",
    "    return torch.sigmoid(iv@coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cedf75-0cec-486d-9603-141197210c6e",
   "metadata": {},
   "source": [
    "## 2) Column vector for coeffs and dv (fn)\n",
    "In order to do matrix-matrix products (which we'll need in the next section), we need to:\n",
    "1) turn coeffs into a column vector (i.e. a matrix with a single column)  \n",
    "We can do it by passing a second argument 1 to torch.rand(), indicating that we want our coefficients to have one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5e36ae4d-6ddf-44f9-96bc-f893f055cf94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0672],\n",
       "        [0.0732],\n",
       "        [0.0142],\n",
       "        [0.0499],\n",
       "        [0.0317],\n",
       "        [0.0259],\n",
       "        [0.0549],\n",
       "        [0.0587],\n",
       "        [0.0447],\n",
       "        [0.0813],\n",
       "        [0.0351],\n",
       "        [0.0245]], requires_grad=True)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_coeffs():\n",
    "    return (torch.rand(n_coeff, 1)*0.1).requires_grad_()\n",
    "init_coeffs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1b0f81-7a20-4396-be2a-d1864897e16e",
   "metadata": {},
   "source": [
    "2) turn our dependent variable into a column vector  \n",
    "We can do it by indexing the column dimension with the special value None, which tells PyTorch to add a new dimension in this position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8c67c86c-7fb5-450b-9db3-8b82839729d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dv = train_dv[:,None]\n",
    "valid_dv = valid_dv[:,None]\n",
    "valid_dv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7628a25c-800c-4f88-b9b2-d874c0994ed3",
   "metadata": {},
   "source": [
    "## 3) Training and results\n",
    "We can now train our model as before and confirm we get identical outputs and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "63dbef73-686b-4fbb-af1b-d718d5fab9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.512; 0.323; 0.290; 0.205; 0.200; 0.198; 0.197; 0.197; 0.196; 0.196; 0.196; 0.195; 0.195; 0.195; 0.195; 0.195; 0.195; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; "
     ]
    }
   ],
   "source": [
    "coeffs = train_model(lr=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f3dd15d5-c7b3-40a2-9ac3-8c1f941864aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8258)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc(coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8170244d-1afc-4343-81cb-22b83e1d86a7",
   "metadata": {},
   "source": [
    "# Step 4. NN\n",
    "## 1) init coeffs (fn): hidden and output layers\n",
    "We've now got what we need to implement our neural network.\n",
    "\n",
    "First, we'll need to create coefficients for each of our layers. \n",
    "1) Our first set of coefficients will take our n_coeff inputs, and create n_hidden outputs. We can choose whatever n_hidden we like -- a higher number gives our network more flexibility, but makes it slower and harder to train. So we need a matrix of size n_coeff by n_hidden. We'll divide these coefficients by n_hidden so that when we sum them up in the next layer we'll end up with similar magnitude numbers to what we started with.\n",
    "2) Then our second layer will need to take the n_hidden inputs and create a single output, so that means we need a n_hidden by 1 matrix there. The second layer will also need a constant term added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "35e1077a-9af8-4093-ae77-96ffb7c40327",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[     0.0140,      0.0202,      0.0145,     -0.0168,      0.0015,      0.0191,      0.0246,     -0.0095,     -0.0009,     -0.0243,\n",
       "               0.0216,     -0.0142,     -0.0176,      0.0109,      0.0057,      0.0020,     -0.0090,     -0.0124,      0.0143,      0.0183],\n",
       "         [     0.0156,      0.0122,     -0.0041,     -0.0202,     -0.0243,      0.0107,     -0.0205,     -0.0034,     -0.0215,     -0.0238,\n",
       "               0.0084,     -0.0194,      0.0189,     -0.0154,      0.0183,      0.0215,     -0.0031,      0.0148,      0.0061,      0.0001],\n",
       "         [    -0.0097,      0.0248,      0.0184,      0.0148,      0.0188,     -0.0216,      0.0049,      0.0056,     -0.0200,     -0.0110,\n",
       "              -0.0096,     -0.0199,     -0.0157,     -0.0205,     -0.0145,      0.0129,     -0.0126,     -0.0016,      0.0092,      0.0158],\n",
       "         [     0.0210,      0.0038,      0.0077,     -0.0035,      0.0072,     -0.0171,      0.0119,      0.0200,     -0.0154,     -0.0131,\n",
       "               0.0121,      0.0002,     -0.0225,      0.0120,      0.0209,     -0.0203,     -0.0017,     -0.0228,      0.0180,      0.0067],\n",
       "         [     0.0223,      0.0123,      0.0171,      0.0177,      0.0041,      0.0025,     -0.0049,     -0.0135,     -0.0022,      0.0062,\n",
       "               0.0082,      0.0134,      0.0173,      0.0166,     -0.0213,      0.0084,      0.0129,      0.0052,      0.0077,      0.0094],\n",
       "         [    -0.0211,     -0.0080,     -0.0096,     -0.0099,     -0.0161,     -0.0193,     -0.0048,      0.0170,     -0.0062,      0.0164,\n",
       "               0.0161,      0.0090,      0.0180,     -0.0050,     -0.0045,     -0.0152,     -0.0055,     -0.0199,     -0.0043,      0.0180],\n",
       "         [     0.0146,     -0.0015,     -0.0099,     -0.0045,     -0.0202,      0.0217,      0.0245,      0.0149,     -0.0228,      0.0099,\n",
       "              -0.0033,      0.0109,     -0.0187,     -0.0005,      0.0131,      0.0094,      0.0157,      0.0144,     -0.0219,      0.0125],\n",
       "         [     0.0118,     -0.0081,     -0.0163,     -0.0191,      0.0129,     -0.0053,      0.0056,     -0.0187,      0.0135,      0.0223,\n",
       "               0.0051,     -0.0114,      0.0019,      0.0122,      0.0039,      0.0104,     -0.0138,     -0.0121,     -0.0152,      0.0250],\n",
       "         [    -0.0131,      0.0163,     -0.0155,     -0.0007,     -0.0204,     -0.0102,      0.0046,     -0.0121,      0.0015,      0.0132,\n",
       "               0.0032,      0.0236,      0.0118,     -0.0200,      0.0174,      0.0133,     -0.0023,      0.0012,      0.0014,      0.0192],\n",
       "         [     0.0150,     -0.0236,      0.0030,      0.0061,      0.0026,     -0.0022,      0.0097,      0.0244,     -0.0103,     -0.0001,\n",
       "               0.0082,     -0.0230,     -0.0129,     -0.0240,      0.0195,     -0.0230,      0.0139,      0.0061,      0.0129,      0.0055],\n",
       "         [    -0.0197,     -0.0092,     -0.0242,     -0.0153,     -0.0210,     -0.0104,     -0.0027,     -0.0004,     -0.0091,      0.0126,\n",
       "              -0.0083,      0.0129,     -0.0084,     -0.0065,      0.0207,     -0.0058,     -0.0185,      0.0118,      0.0230,     -0.0008],\n",
       "         [     0.0234,     -0.0012,     -0.0041,      0.0167,      0.0127,      0.0101,      0.0140,      0.0121,      0.0054,      0.0204,\n",
       "               0.0058,     -0.0229,      0.0031,      0.0036,      0.0096,      0.0010,     -0.0172,      0.0134,      0.0123,     -0.0179]],\n",
       "        requires_grad=True),\n",
       " tensor([[-0.0078],\n",
       "         [ 0.2310],\n",
       "         [ 0.1101],\n",
       "         [-0.1141],\n",
       "         [ 0.1249],\n",
       "         [ 0.1606],\n",
       "         [-0.0506],\n",
       "         [ 0.5773],\n",
       "         [-0.0917],\n",
       "         [-0.2673],\n",
       "         [ 0.4028],\n",
       "         [-0.2659],\n",
       "         [ 0.4839],\n",
       "         [ 0.6552],\n",
       "         [-0.1122],\n",
       "         [-0.0507],\n",
       "         [-0.2526],\n",
       "         [ 0.1559],\n",
       "         [ 0.6932],\n",
       "         [-0.2423]], requires_grad=True),\n",
       " tensor(0.1341, requires_grad=True))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_coeffs(n_hidden=20):\n",
    "    layer1 = (torch.rand(n_coeff, n_hidden)-0.5)/n_hidden\n",
    "    layer2 = torch.rand(n_hidden, 1)-0.3\n",
    "    const = torch.rand(1)[0]\n",
    "    return layer1.requires_grad_(), layer2.requires_grad_(), const.requires_grad_()\n",
    "init_coeffs()\n",
    "# layer1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ec95cc-ba13-454a-9b13-45459149a594",
   "metadata": {},
   "source": [
    "## 2) Calc preds (fn): matrix mult\n",
    "Now we have our coefficients, we can create our neural net. The key steps are the two matrix products, iv@l1 and a@l2 (where a is the output of the first layer). The first layer output is passed to F.relu (that's our non-linearity), and the second is passed to torch.sigmoid as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c063509e-af4e-4420-bcc8-4e4263d3925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_preds(coeffs, iv): \n",
    "    l1, l2, const = coeffs\n",
    "    a = F.relu(iv@l1)\n",
    "    a = torch.sigmoid(a@l2 + const)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142fa2ad-04f0-436a-96e8-aa8774e34596",
   "metadata": {},
   "source": [
    "## 3) Update coeff (fn): 2 layers\n",
    "Finally, now that we have more than one set of coefficients, we need to add a loop to update each one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "561db705-82db-485e-ac86-61df95058eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_coeffs(coeffs, lr):\n",
    "    for layer in coeffs:\n",
    "        layer.sub_(layer.grad * lr)\n",
    "        layer.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dc4231-cd55-4afc-aeb7-c5c1f48836c2",
   "metadata": {},
   "source": [
    "## 4) Train NN\n",
    "It's looking good -- our loss is lower than before. Let's see if that translates to a better result on the validation set. In this case our neural net isn't showing better results than the linear model. That's not surprising; this dataset is very small and very simple, and isn't the kind of thing we'd expect to see neural networks excel at. Furthermore, our validation set is too small to reliably see much accuracy difference. But the key thing is that we now know exactly what a real neural net looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "da001d85-df50-4db8-9143-dbf6473f3700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.543; 0.400; 0.260; 0.390; 0.221; 0.211; 0.197; 0.195; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; "
     ]
    }
   ],
   "source": [
    "coeffs = train_model(lr=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "411b2b5d-8bab-4ab3-a092-c91c4f1bc25e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8258)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc(coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b7e40e-f3c4-415b-a23a-4678bdde286e",
   "metadata": {},
   "source": [
    "# Step 5. Deep learning\n",
    "## 1) Init coeffs (fn)\n",
    "NN in the previous section only uses one hidden layer, so it doesn't count as \"deep\" learning. But we can use the exact same technique to make our NN deep, by adding more matrix multiplications.\n",
    "First, we'll need to create additional coefficients for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bbf04c0f-085d-4067-a998-527a01a7f4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_coeffs():\n",
    "    hiddens = [10, 10]  # <-- set this to the size of each hidden layer you want\n",
    "    sizes = [n_coeff] + hiddens + [1]\n",
    "    n = len(sizes)\n",
    "    layers = [(torch.rand(sizes[i], sizes[i+1])-0.3)/sizes[i+1]*4 for i in range(n-1)]\n",
    "    consts = [(torch.rand(1)[0]-0.5)*0.1 for i in range(n-1)]\n",
    "    for l in layers+consts: l.requires_grad_()\n",
    "    return layers,consts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc098431-fd57-44d9-a1ed-0d10b512415a",
   "metadata": {},
   "source": [
    "## 2) Calc preds (fn)\n",
    "Our deep learning calc_preds looks much the same as before, but now we loop through each layer, instead of listing them separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1ecdd2b4-77e4-4ffa-8061-36cbf0ba143e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_preds(coeffs, iv):\n",
    "    layers, consts = coeffs\n",
    "    n = len(layers)\n",
    "    a = iv\n",
    "    for i,l in enumerate(layers):\n",
    "        a = a@l + consts[i]\n",
    "        if i!=n-1: a = F.relu(a)\n",
    "    return torch.sigmoid(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dd9ccd-171d-48b1-a3b6-8fac3012606a",
   "metadata": {},
   "source": [
    "## 3) Update coeffs (fn)\n",
    "We also need a minor update to update_coeffs since we've got layers and consts separated now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7546f1f8-6258-4f50-ba15-ae6b41968006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_coeffs(coeffs, lr):\n",
    "    layers, consts = coeffs\n",
    "    for layer in layers + consts:\n",
    "        layer.sub_(layer.grad * lr)\n",
    "        layer.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e767a6-4ffc-43fd-bcfb-554561a51ffd",
   "metadata": {},
   "source": [
    "## 4) Train NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bf5e2e60-e07d-4456-849b-bfd82a6cc8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.521; 0.483; 0.427; 0.379; 0.379; 0.379; 0.379; 0.378; 0.378; 0.378; 0.378; 0.378; 0.378; 0.378; 0.378; 0.378; 0.377; 0.376; 0.371; 0.333; 0.239; 0.224; 0.208; 0.204; 0.203; 0.203; 0.207; 0.197; 0.196; 0.195; "
     ]
    }
   ],
   "source": [
    "coeffs = train_model(lr=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "04d4b9a8-54bf-4584-b518-74a240ab7f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8258)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc(coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e61d3d-ab3d-420e-b93a-479878d67526",
   "metadata": {},
   "source": [
    "# Step 6. Fastai framework\n",
    "Now is a good time to look at how to do the same thing using a library, instead of doing it from scratch. We'll use fastai and PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "28f24613-9b49-4411-bb0c-df8d7e75f56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.tabular.all import *\n",
    "\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa656a9-b8cb-4df1-bcc7-1070b30e2e5c",
   "metadata": {},
   "source": [
    "## 1) Data preparation\n",
    "linkcode\n",
    "When you do everything from scratch, every bit of feature engineering requires a whole lot of work, since you have to think about things like dummy variables, normalization, missing values, and so on. But with fastai that's all done for you. So let's go wild and create lots of new features! We'll use a bunch of the most interesting ones from this fantastic \"Titanic - Advanced Feature Engineering Tutorial\" (https://www.kaggle.com/code/gunesevitan/titanic-advanced-feature-engineering-tutorial/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0240617a-e23f-473d-a7af-cd0205eb65a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/Users/hela/Code/fast_ai/titanic/train.csv')\n",
    "\n",
    "def add_features(train_df):\n",
    "    train_df['LogFare'] = np.log1p(train_df['Fare'])\n",
    "    train_df['Deck'] = train_df.Cabin.str[0].map(dict(A=\"ABC\", B=\"ABC\", C=\"ABC\", D=\"DE\", E=\"DE\", F=\"FG\", G=\"FG\"))\n",
    "    train_df['Family'] = train_df.SibSp+train_df.Parch\n",
    "    train_df['Alone'] = train_df.Family==0\n",
    "    train_df['TicketFreq'] = train_df.groupby('Ticket')['Ticket'].transform('count')\n",
    "    train_df['Title'] = train_df.Name.str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\n",
    "    train_df['Title'] = train_df.Title.map(dict(Mr=\"Mr\",Miss=\"Miss\",Mrs=\"Mrs\",Master=\"Master\"))\n",
    "add_features(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d0b6c6-62e7-4961-9d27-b6b07f9e5244",
   "metadata": {},
   "source": [
    "Use RandomSplitter to separate out the training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7b7f5ab6-e338-4878-9bb5-bdfdd09957d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = RandomSplitter(seed=42)(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53136906-5946-43cf-ab4e-e34161485904",
   "metadata": {},
   "source": [
    "Now the entire process of getting the data ready for training requires just this one cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e3bed50e-66a6-4f5c-bb52-6ac38169ae34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hela/Code/fast_ai/.venv/lib/python3.13/site-packages/fastai/tabular/core.py:314: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  to[n].fillna(self.na_dict[n], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "dls = TabularPandas(\n",
    "    train_df, splits=splits,  #  indices of training and validation sets\n",
    "    procs = [Categorify, FillMissing, Normalize],  # strings into categ, fill NAs in num columns with Median, normalise num columns\n",
    "    cat_names=[\"Sex\",\"Pclass\",\"Embarked\",\"Deck\", \"Title\"],  # categorical IVs\n",
    "    cont_names=['Age', 'SibSp', 'Parch', 'LogFare', 'Alone', 'TicketFreq', 'Family'],  # continous IVs\n",
    "    y_names=\"Survived\", y_block = CategoryBlock(),  # DV; it is categ (so build a classification model, not a regression model)\n",
    ").dataloaders(path=\".\")  # save in the same file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05afe0a-4840-4cfd-a365-a199d98a837c",
   "metadata": {},
   "source": [
    "## 2) Train the model\n",
    "The data and model together make up a Learner. To create one, we say what the data is (dls), and the size of each hidden layer ([10,10]), along with any metrics we want to print along the way.  \n",
    "You'll notice we didn't have to do any messing around to try to find a set of random coefficients that will train correctly -- that's all handled automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "02e18f6d-c81b-4c9b-a747-61d53db61b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = tabular_learner(dls, metrics=accuracy, layers=[10,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fd3b58-cdf6-45ae-9883-492b617ffc93",
   "metadata": {},
   "source": [
    "One handy feature that fastai can also tell us what learning rate to use. The two colored points are both reasonable choices for a learning rate. I'll pick somewhere between the two (0.03) because it usually gives the best results. train for a few epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e4368b8e-9a20-45ba-b7b9-7a4d1929f07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(slide=0.03981071710586548, valley=0.015848932787775993)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find(suggest_funcs=(slide, valley))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464d3c53-f5aa-47a5-bc9d-53787b81cf46",
   "metadata": {},
   "source": [
    "Train the model.\n",
    "We've got a similar accuracy to our previous \"from scratch\" model -- which isn't too surprising, since as we discussed, this dataset is too small and simple to really see much difference. A simple linear model already does a pretty good job. But that's OK -- the goal here is to show you how to get started with deep learning and understand how it really works, and the best way to do that is on small and easy to understand datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f8cfa42d-09cb-426e-a30c-3ddef93128bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.404043</td>\n",
       "      <td>0.424650</td>\n",
       "      <td>0.825843</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.396436</td>\n",
       "      <td>0.415919</td>\n",
       "      <td>0.808989</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.380295</td>\n",
       "      <td>0.417379</td>\n",
       "      <td>0.814607</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.384151</td>\n",
       "      <td>0.427768</td>\n",
       "      <td>0.797753</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.386731</td>\n",
       "      <td>0.414579</td>\n",
       "      <td>0.814607</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.388829</td>\n",
       "      <td>0.408999</td>\n",
       "      <td>0.808989</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.379542</td>\n",
       "      <td>0.436101</td>\n",
       "      <td>0.808989</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.377454</td>\n",
       "      <td>0.438198</td>\n",
       "      <td>0.803371</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.390365</td>\n",
       "      <td>0.420966</td>\n",
       "      <td>0.803371</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.390907</td>\n",
       "      <td>0.415635</td>\n",
       "      <td>0.808989</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.390799</td>\n",
       "      <td>0.415521</td>\n",
       "      <td>0.814607</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.382446</td>\n",
       "      <td>0.453546</td>\n",
       "      <td>0.803371</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.385244</td>\n",
       "      <td>0.450956</td>\n",
       "      <td>0.792135</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.383339</td>\n",
       "      <td>0.438377</td>\n",
       "      <td>0.792135</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.381524</td>\n",
       "      <td>0.469439</td>\n",
       "      <td>0.820225</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.383545</td>\n",
       "      <td>0.429202</td>\n",
       "      <td>0.803371</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(16, lr=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22dfaa1-d333-4590-8002-6b164341a6c8",
   "metadata": {},
   "source": [
    "## 3) Final submission\n",
    "One important feature of fastai is that all the information needed to apply the data transformations and the model to a new dataset are stored in the learner. You can call export to save it to a file to use it later in production, or you can use the trained model right away to get predictions on a test set.\n",
    "\n",
    "To submit to Kaggle, we'll need to read in the test set, and do the same feature engineering we did for the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8c3968e1-9455-42e9-8cf6-65aef3f66758",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('/Users/hela/Code/fast_ai/titanic/test.csv')\n",
    "test_df['Fare'] = test_df.Fare.fillna(0)\n",
    "add_features(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d45ab3c-57b7-4816-97fa-5592c223710a",
   "metadata": {},
   "source": [
    "But we don't need to manually specify any of the processing steps necessary to get the data ready for modeling, since that's all saved in the learner. To specify we want to apply the same steps to a new dataset, use the test_dl() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a59feebc-98f3-4ff9-9858-2b29506fccb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hela/Code/fast_ai/.venv/lib/python3.13/site-packages/fastai/tabular/core.py:314: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  to[n].fillna(self.na_dict[n], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "test_dl = learn.dls.test_dl(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585fc638-006e-4f95-84ec-f4c21a461809",
   "metadata": {},
   "source": [
    "Now we can use get_preds to get the predictions for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9c6d337e-2702-4477-8bc1-a6a3b9b912c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[    0.8713,     0.1287],\n",
       "         [    0.6754,     0.3246],\n",
       "         [    0.9069,     0.0931],\n",
       "         [    0.9020,     0.0980],\n",
       "         [    0.1747,     0.8253],\n",
       "         [    0.8813,     0.1187],\n",
       "         [    0.5895,     0.4105],\n",
       "         [    0.9067,     0.0933],\n",
       "         [    0.3697,     0.6303],\n",
       "         [    0.9164,     0.0836],\n",
       "         [    0.9081,     0.0919],\n",
       "         [    0.9186,     0.0814],\n",
       "         [    0.0378,     0.9622],\n",
       "         [    0.9146,     0.0854],\n",
       "         [    0.0208,     0.9792],\n",
       "         [    0.0162,     0.9838],\n",
       "         [    0.8613,     0.1387],\n",
       "         [    0.8310,     0.1690],\n",
       "         [    0.6351,     0.3649],\n",
       "         [    0.6239,     0.3761],\n",
       "         [    0.8632,     0.1368],\n",
       "         [    0.0261,     0.9739],\n",
       "         [    0.0560,     0.9440],\n",
       "         [    0.7799,     0.2201],\n",
       "         [    0.1971,     0.8029],\n",
       "         [    0.9099,     0.0901],\n",
       "         [    0.0051,     0.9949],\n",
       "         [    0.8345,     0.1655],\n",
       "         [    0.8569,     0.1431],\n",
       "         [    0.8896,     0.1104],\n",
       "         [    0.9024,     0.0976],\n",
       "         [    0.9084,     0.0916],\n",
       "         [    0.3353,     0.6647],\n",
       "         [    0.3043,     0.6957],\n",
       "         [    0.6287,     0.3713],\n",
       "         [    0.8251,     0.1749],\n",
       "         [    0.7593,     0.2407],\n",
       "         [    0.7093,     0.2907],\n",
       "         [    0.8991,     0.1009],\n",
       "         [    0.9083,     0.0917],\n",
       "         [    0.8493,     0.1507],\n",
       "         [    0.2579,     0.7421],\n",
       "         [    0.9208,     0.0792],\n",
       "         [    0.0789,     0.9211],\n",
       "         [    0.0188,     0.9812],\n",
       "         [    0.8990,     0.1010],\n",
       "         [    0.6648,     0.3352],\n",
       "         [    0.8651,     0.1349],\n",
       "         [    0.0737,     0.9263],\n",
       "         [    0.1223,     0.8777],\n",
       "         [    0.7634,     0.2366],\n",
       "         [    0.8318,     0.1682],\n",
       "         [    0.0078,     0.9922],\n",
       "         [    0.0077,     0.9923],\n",
       "         [    0.8414,     0.1586],\n",
       "         [    0.0255,     0.9745],\n",
       "         [    0.9132,     0.0868],\n",
       "         [    0.8756,     0.1244],\n",
       "         [    0.8894,     0.1106],\n",
       "         [    0.1185,     0.8815],\n",
       "         [    0.8864,     0.1136],\n",
       "         [    0.9010,     0.0990],\n",
       "         [    0.8880,     0.1120],\n",
       "         [    0.4908,     0.5092],\n",
       "         [    0.0408,     0.9592],\n",
       "         [    0.2008,     0.7992],\n",
       "         [    0.4469,     0.5531],\n",
       "         [    0.9199,     0.0801],\n",
       "         [    0.4797,     0.5203],\n",
       "         [    0.0022,     0.9978],\n",
       "         [    0.5129,     0.4871],\n",
       "         [    0.8928,     0.1072],\n",
       "         [    0.7440,     0.2560],\n",
       "         [    0.1034,     0.8966],\n",
       "         [    0.1320,     0.8680],\n",
       "         [    0.5410,     0.4590],\n",
       "         [    0.9081,     0.0919],\n",
       "         [    0.0352,     0.9648],\n",
       "         [    0.8980,     0.1020],\n",
       "         [    0.5129,     0.4871],\n",
       "         [    0.0187,     0.9813],\n",
       "         [    0.8712,     0.1288],\n",
       "         [    0.9223,     0.0777],\n",
       "         [    0.9081,     0.0919],\n",
       "         [    0.8536,     0.1464],\n",
       "         [    0.8365,     0.1635],\n",
       "         [    0.5485,     0.4515],\n",
       "         [    0.6948,     0.3052],\n",
       "         [    0.6888,     0.3112],\n",
       "         [    0.0019,     0.9981],\n",
       "         [    0.6199,     0.3801],\n",
       "         [    0.9081,     0.0919],\n",
       "         [    0.0011,     0.9989],\n",
       "         [    0.9081,     0.0919],\n",
       "         [    0.8245,     0.1755],\n",
       "         [    0.8990,     0.1010],\n",
       "         [    0.0736,     0.9264],\n",
       "         [    0.9049,     0.0951],\n",
       "         [    0.7045,     0.2955],\n",
       "         [    0.9105,     0.0895],\n",
       "         [    0.0390,     0.9610],\n",
       "         [    0.8725,     0.1275],\n",
       "         [    0.8651,     0.1349],\n",
       "         [    0.9005,     0.0995],\n",
       "         [    0.1247,     0.8753],\n",
       "         [    0.9168,     0.0832],\n",
       "         [    0.8439,     0.1561],\n",
       "         [    0.8651,     0.1349],\n",
       "         [    0.9081,     0.0919],\n",
       "         [    0.8518,     0.1482],\n",
       "         [    0.8620,     0.1380],\n",
       "         [    0.5907,     0.4093],\n",
       "         [    0.0428,     0.9572],\n",
       "         [    0.4523,     0.5477],\n",
       "         [    0.2853,     0.7147],\n",
       "         [    0.8122,     0.1878],\n",
       "         [    0.8536,     0.1464],\n",
       "         [    0.1126,     0.8874],\n",
       "         [    0.6700,     0.3300],\n",
       "         [    0.0223,     0.9777],\n",
       "         [    0.0746,     0.9254],\n",
       "         [    0.8502,     0.1498],\n",
       "         [    0.0275,     0.9725],\n",
       "         [    0.9035,     0.0965],\n",
       "         [    0.8651,     0.1349],\n",
       "         [    0.2646,     0.7354],\n",
       "         [    0.8944,     0.1056],\n",
       "         [    0.7036,     0.2964],\n",
       "         [    0.9149,     0.0851],\n",
       "         [    0.8975,     0.1025],\n",
       "         [    0.9091,     0.0909],\n",
       "         [    0.5378,     0.4622],\n",
       "         [    0.0109,     0.9891],\n",
       "         [    0.8393,     0.1607],\n",
       "         [    0.9232,     0.0768],\n",
       "         [    0.8975,     0.1025],\n",
       "         [    0.8435,     0.1565],\n",
       "         [    0.8917,     0.1083],\n",
       "         [    0.7183,     0.2817],\n",
       "         [    0.3593,     0.6407],\n",
       "         [    0.0563,     0.9437],\n",
       "         [    0.2330,     0.7670],\n",
       "         [    0.9549,     0.0451],\n",
       "         [    0.8949,     0.1051],\n",
       "         [    0.9135,     0.0865],\n",
       "         [    0.9521,     0.0479],\n",
       "         [    0.2581,     0.7419],\n",
       "         [    0.8944,     0.1056],\n",
       "         [    0.8982,     0.1018],\n",
       "         [    0.3116,     0.6884],\n",
       "         [    0.0029,     0.9971],\n",
       "         [    0.8536,     0.1464],\n",
       "         [    0.9414,     0.0586],\n",
       "         [    0.4280,     0.5720],\n",
       "         [    0.0582,     0.9418],\n",
       "         [    0.8975,     0.1025],\n",
       "         [    0.1761,     0.8239],\n",
       "         [    0.7183,     0.2817],\n",
       "         [    0.4246,     0.5754],\n",
       "         [    0.6792,     0.3208],\n",
       "         [    0.5907,     0.4093],\n",
       "         [    0.0830,     0.9170],\n",
       "         [    0.1102,     0.8898],\n",
       "         [    0.9081,     0.0919],\n",
       "         [    0.8890,     0.1110],\n",
       "         [    0.6044,     0.3956],\n",
       "         [    0.6973,     0.3027],\n",
       "         [    0.8818,     0.1182],\n",
       "         [    0.0464,     0.9536],\n",
       "         [    0.7140,     0.2860],\n",
       "         [    0.9081,     0.0919],\n",
       "         [    0.8446,     0.1554],\n",
       "         [    0.8740,     0.1260],\n",
       "         [    0.8536,     0.1464],\n",
       "         [    0.9551,     0.0449],\n",
       "         [    0.0023,     0.9977],\n",
       "         [    0.2256,     0.7744],\n",
       "         [    0.7382,     0.2618],\n",
       "         [    0.0042,     0.9958],\n",
       "         [    0.0194,     0.9806],\n",
       "         [    0.8980,     0.1020],\n",
       "         [    0.6273,     0.3727],\n",
       "         [    0.0048,     0.9952],\n",
       "         [    0.8651,     0.1349],\n",
       "         [    0.0030,     0.9970],\n",
       "         [    0.9123,     0.0877],\n",
       "         [    0.0428,     0.9572],\n",
       "         [    0.9122,     0.0878],\n",
       "         [    0.5076,     0.4924],\n",
       "         [    0.9123,     0.0877],\n",
       "         [    0.8823,     0.1177],\n",
       "         [    0.8982,     0.1018],\n",
       "         [    0.1113,     0.8887],\n",
       "         [    0.9055,     0.0945],\n",
       "         [    0.0009,     0.9991],\n",
       "         [    0.9105,     0.0895],\n",
       "         [    0.0013,     0.9987],\n",
       "         [    0.6948,     0.3052],\n",
       "         [    0.8868,     0.1132],\n",
       "         [    0.7364,     0.2636],\n",
       "         [    0.5915,     0.4085],\n",
       "         [    0.0057,     0.9943],\n",
       "         [    0.5425,     0.4575],\n",
       "         [    0.0049,     0.9951],\n",
       "         [    0.8901,     0.1099],\n",
       "         [    0.8390,     0.1610],\n",
       "         [    0.6324,     0.3676],\n",
       "         [    0.8884,     0.1116],\n",
       "         [    0.0359,     0.9641],\n",
       "         [    0.8990,     0.1010],\n",
       "         [    0.9206,     0.0794],\n",
       "         [    0.9081,     0.0919],\n",
       "         [    0.8991,     0.1009],\n",
       "         [    0.1982,     0.8018],\n",
       "         [    0.0579,     0.9421],\n",
       "         [    0.8590,     0.1410],\n",
       "         [    0.5908,     0.4092],\n",
       "         [    0.9455,     0.0545],\n",
       "         [    0.1985,     0.8015],\n",
       "         [    0.9081,     0.0919],\n",
       "         [    0.0381,     0.9619],\n",
       "         [    0.8928,     0.1072],\n",
       "         [    0.0557,     0.9443],\n",
       "         [    0.8928,     0.1072],\n",
       "         [    0.0876,     0.9124],\n",
       "         [    0.0895,     0.9105],\n",
       "         [    0.8960,     0.1040],\n",
       "         [    0.5907,     0.4093],\n",
       "         [    0.9202,     0.0798],\n",
       "         [    0.9068,     0.0932],\n",
       "         [    0.8967,     0.1033],\n",
       "         [    0.0668,     0.9332],\n",
       "         [    0.8735,     0.1265],\n",
       "         [    0.8651,     0.1349],\n",
       "         [    0.6881,     0.3119],\n",
       "         [    0.8912,     0.1088],\n",
       "         [    0.5559,     0.4441],\n",
       "         [    0.8287,     0.1713],\n",
       "         [    0.0077,     0.9923],\n",
       "         [    0.0237,     0.9763],\n",
       "         [    0.0931,     0.9069],\n",
       "         [    0.0051,     0.9949],\n",
       "         [    0.6509,     0.3491],\n",
       "         [    0.9081,     0.0919],\n",
       "         [    0.0856,     0.9144],\n",
       "         [    0.6168,     0.3832],\n",
       "         [    0.1585,     0.8415],\n",
       "         [    0.9354,     0.0646],\n",
       "         [    0.0223,     0.9777],\n",
       "         [    0.5287,     0.4713],\n",
       "         [    0.0011,     0.9989],\n",
       "         [    0.8912,     0.1088],\n",
       "         [    0.6206,     0.3794],\n",
       "         [    0.8975,     0.1025],\n",
       "         [    0.9098,     0.0902],\n",
       "         [    0.9081,     0.0919],\n",
       "         [    0.8651,     0.1349],\n",
       "         [    0.9035,     0.0965],\n",
       "         [    0.0905,     0.9095],\n",
       "         [    0.8928,     0.1072],\n",
       "         [    0.8944,     0.1056],\n",
       "         [    0.8928,     0.1072],\n",
       "         [    0.0110,     0.9890],\n",
       "         [    0.4320,     0.5680],\n",
       "         [    0.8386,     0.1614],\n",
       "         [    0.9081,     0.0919],\n",
       "         [    0.8979,     0.1021],\n",
       "         [    0.9081,     0.0919],\n",
       "         [    0.7593,     0.2407],\n",
       "         [    0.8864,     0.1136],\n",
       "         [    0.7513,     0.2487],\n",
       "         [    0.8651,     0.1349],\n",
       "         [    0.0185,     0.9815],\n",
       "         [    0.5967,     0.4033],\n",
       "         [    0.8536,     0.1464],\n",
       "         [    0.0267,     0.9733],\n",
       "         [    0.8949,     0.1051],\n",
       "         [    0.8902,     0.1098],\n",
       "         [    0.8743,     0.1257],\n",
       "         [    0.8851,     0.1149],\n",
       "         [    0.7184,     0.2816],\n",
       "         [    0.2607,     0.7393],\n",
       "         [    0.5907,     0.4093],\n",
       "         [    0.5159,     0.4841],\n",
       "         [    0.1323,     0.8677],\n",
       "         [    0.9145,     0.0855],\n",
       "         [    0.9081,     0.0919],\n",
       "         [    0.7635,     0.2365],\n",
       "         [    0.8218,     0.1782],\n",
       "         [    0.9081,     0.0919],\n",
       "         [    0.8983,     0.1017],\n",
       "         [    0.5894,     0.4106],\n",
       "         [    0.8536,     0.1464],\n",
       "         [    0.7983,     0.2017],\n",
       "         [    0.9145,     0.0855],\n",
       "         [    0.9005,     0.0995],\n",
       "         [    0.0010,     0.9990],\n",
       "         [    0.8896,     0.1104],\n",
       "         [    0.8964,     0.1036],\n",
       "         [    0.9049,     0.0951],\n",
       "         [    0.9091,     0.0909],\n",
       "         [    0.1081,     0.8919],\n",
       "         [    0.9025,     0.0975],\n",
       "         [    0.8975,     0.1025],\n",
       "         [    0.5907,     0.4093],\n",
       "         [    0.0187,     0.9813],\n",
       "         [    0.9025,     0.0975],\n",
       "         [    0.0180,     0.9820],\n",
       "         [    0.9113,     0.0887],\n",
       "         [    0.6613,     0.3387],\n",
       "         [    0.8880,     0.1120],\n",
       "         [    0.8334,     0.1666],\n",
       "         [    0.9081,     0.0919],\n",
       "         [    0.6431,     0.3569],\n",
       "         [    0.0898,     0.9102],\n",
       "         [    0.4360,     0.5640],\n",
       "         [    0.7676,     0.2324],\n",
       "         [    0.8799,     0.1201],\n",
       "         [    0.9020,     0.0980],\n",
       "         [    0.9062,     0.0938],\n",
       "         [    0.9005,     0.0995],\n",
       "         [    0.8061,     0.1939],\n",
       "         [    0.8669,     0.1331],\n",
       "         [    0.9009,     0.0991],\n",
       "         [    0.1146,     0.8854],\n",
       "         [    0.8960,     0.1040],\n",
       "         [    0.0209,     0.9791],\n",
       "         [    0.8697,     0.1303],\n",
       "         [    0.8754,     0.1246],\n",
       "         [    0.8834,     0.1166],\n",
       "         [    0.0263,     0.9737],\n",
       "         [    0.5898,     0.4102],\n",
       "         [    0.8536,     0.1464],\n",
       "         [    0.1410,     0.8590],\n",
       "         [    0.9020,     0.0980],\n",
       "         [    0.8329,     0.1671],\n",
       "         [    0.9010,     0.0990],\n",
       "         [    0.9029,     0.0971],\n",
       "         [    0.9087,     0.0913],\n",
       "         [    0.4509,     0.5491],\n",
       "         [    0.8781,     0.1219],\n",
       "         [    0.9091,     0.0909],\n",
       "         [    0.0738,     0.9262],\n",
       "         [    0.0143,     0.9857],\n",
       "         [    0.2297,     0.7703],\n",
       "         [    0.6849,     0.3151],\n",
       "         [    0.8917,     0.1083],\n",
       "         [    0.5855,     0.4145],\n",
       "         [    0.8884,     0.1116],\n",
       "         [    0.1276,     0.8724],\n",
       "         [    0.0053,     0.9947],\n",
       "         [    0.8901,     0.1099],\n",
       "         [    0.9002,     0.0998],\n",
       "         [    0.9652,     0.0348],\n",
       "         [    0.1658,     0.8342],\n",
       "         [    0.5443,     0.4557],\n",
       "         [    0.0422,     0.9578],\n",
       "         [    0.9081,     0.0919],\n",
       "         [    0.8651,     0.1349],\n",
       "         [    0.5456,     0.4544],\n",
       "         [    0.0395,     0.9605],\n",
       "         [    0.0050,     0.9950],\n",
       "         [    0.2025,     0.7975],\n",
       "         [    0.9020,     0.0980],\n",
       "         [    0.0032,     0.9968],\n",
       "         [    0.0003,     0.9997],\n",
       "         [    0.8395,     0.1605],\n",
       "         [    0.7150,     0.2850],\n",
       "         [    0.0325,     0.9675],\n",
       "         [    0.8364,     0.1636],\n",
       "         [    0.8634,     0.1366],\n",
       "         [    0.0786,     0.9214],\n",
       "         [    0.8721,     0.1279],\n",
       "         [    0.9175,     0.0825],\n",
       "         [    0.0515,     0.9485],\n",
       "         [    0.2874,     0.7126],\n",
       "         [    0.5744,     0.4256],\n",
       "         [    0.8834,     0.1166],\n",
       "         [    0.8833,     0.1167],\n",
       "         [    0.0454,     0.9546],\n",
       "         [    0.8651,     0.1349],\n",
       "         [    0.8546,     0.1454],\n",
       "         [    0.7368,     0.2632],\n",
       "         [    0.4102,     0.5898],\n",
       "         [    0.8998,     0.1002],\n",
       "         [    0.0628,     0.9372],\n",
       "         [    0.8975,     0.1025],\n",
       "         [    0.9325,     0.0675],\n",
       "         [    0.8439,     0.1561],\n",
       "         [    0.0175,     0.9825],\n",
       "         [    0.8407,     0.1593],\n",
       "         [    0.0084,     0.9916],\n",
       "         [    0.0100,     0.9900],\n",
       "         [    0.9212,     0.0788],\n",
       "         [    0.9713,     0.0287],\n",
       "         [    0.0322,     0.9678],\n",
       "         [    0.8504,     0.1496],\n",
       "         [    0.0080,     0.9920],\n",
       "         [    0.8944,     0.1056],\n",
       "         [    0.8646,     0.1354],\n",
       "         [    0.1158,     0.8842],\n",
       "         [    0.8876,     0.1124],\n",
       "         [    0.0252,     0.9748],\n",
       "         [    0.8745,     0.1255],\n",
       "         [    0.4979,     0.5021],\n",
       "         [    0.0744,     0.9256],\n",
       "         [    0.8664,     0.1336],\n",
       "         [    0.8603,     0.1397],\n",
       "         [    0.5907,     0.4093],\n",
       "         [    0.7207,     0.2793],\n",
       "         [    0.5907,     0.4093],\n",
       "         [    0.0241,     0.9759],\n",
       "         [    0.7398,     0.2602],\n",
       "         [    0.9081,     0.0919],\n",
       "         [    0.1288,     0.8712],\n",
       "         [    0.9177,     0.0823],\n",
       "         [    0.9081,     0.0919],\n",
       "         [    0.0554,     0.9446]]),\n",
       " None)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds,_ = learn.get_preds(dl=test_dl)\n",
    "preds,_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34821ca-b605-4bc7-8f15-2be6e728dae9",
   "metadata": {},
   "source": [
    "Finally, let's create a submission CSV just like we did in the previous notebook and check that it looks reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b2e9ebcb-51f9-4bcc-907a-cd774eaad03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Survived'] = (preds[:,1]>0.5).int()\n",
    "subm_df = test_df[['PassengerId','Survived']]\n",
    "subm_df.to_csv('subm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9bae8647-9dc5-47be-ac7f-7ed141ed5ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId,Survived\n",
      "892,0\n",
      "893,0\n",
      "894,0\n",
      "895,0\n",
      "896,1\n",
      "897,0\n",
      "898,0\n",
      "899,0\n",
      "900,1\n"
     ]
    }
   ],
   "source": [
    "!head subm.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6521afe-4aa2-4a81-87fa-bafa21a89c1a",
   "metadata": {},
   "source": [
    "# Step 7. Ensembling\n",
    "Since it's so easy to create a model now, it's easier to play with more advanced modeling approaches. For instance, we can create five separate models, each trained from different random starting points, and average them. This is the simplest approach of ensembling models, which combines multiple models to generate predictions that are better than any of the single models in the ensemble.\n",
    "\n",
    "To create our ensemble, first we copy the three steps we used above to create and train a model, and apply it to the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f1cf3ed7-c61d-4224-aa22-6058a25b4fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble():\n",
    "    learn = tabular_learner(dls, metrics=accuracy, layers=[10,10])\n",
    "    with learn.no_bar(),learn.no_logging(): learn.fit(16, lr=0.03)\n",
    "    return learn.get_preds(dl=test_dl)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c9825e-7cbe-488e-8729-1065638b2ee7",
   "metadata": {},
   "source": [
    "Run this five times, and collect the results into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a29971be-613d-4bd2-a115-31e7c167866f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learns = [ensemble() for _ in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebbdc97-da80-478b-94e0-8d4f707a62f0",
   "metadata": {},
   "source": [
    "Stack this predictions together and take their average predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2b26898c-ea01-49e6-b7b3-7c6f5f420ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8905, 0.1095],\n",
       "        [0.5057, 0.4943],\n",
       "        [0.9212, 0.0788],\n",
       "        [0.9019, 0.0981],\n",
       "        [0.2821, 0.7179],\n",
       "        [0.8875, 0.1125],\n",
       "        [0.3428, 0.6572],\n",
       "        [0.9130, 0.0870],\n",
       "        [0.1421, 0.8579],\n",
       "        [0.9123, 0.0877],\n",
       "        [0.9201, 0.0799],\n",
       "        [0.8722, 0.1278],\n",
       "        [0.0478, 0.9522],\n",
       "        [0.9384, 0.0616],\n",
       "        [0.0412, 0.9588],\n",
       "        [0.0349, 0.9651],\n",
       "        [0.8762, 0.1238],\n",
       "        [0.7699, 0.2301],\n",
       "        [0.5099, 0.4901],\n",
       "        [0.2861, 0.7139],\n",
       "        [0.7800, 0.2200],\n",
       "        [0.2467, 0.7533],\n",
       "        [0.0572, 0.9428],\n",
       "        [0.7021, 0.2979],\n",
       "        [0.2028, 0.7972],\n",
       "        [0.9423, 0.0577],\n",
       "        [0.0397, 0.9603],\n",
       "        [0.7744, 0.2256],\n",
       "        [0.6866, 0.3134],\n",
       "        [0.8847, 0.1153],\n",
       "        [0.9358, 0.0642],\n",
       "        [0.9176, 0.0824],\n",
       "        [0.4229, 0.5771],\n",
       "        [0.3718, 0.6282],\n",
       "        [0.5131, 0.4869],\n",
       "        [0.7650, 0.2350],\n",
       "        [0.4105, 0.5895],\n",
       "        [0.4211, 0.5789],\n",
       "        [0.8993, 0.1007],\n",
       "        [0.8447, 0.1553],\n",
       "        [0.9271, 0.0729],\n",
       "        [0.2664, 0.7336],\n",
       "        [0.9175, 0.0825],\n",
       "        [0.0701, 0.9299],\n",
       "        [0.0400, 0.9600],\n",
       "        [0.9033, 0.0967],\n",
       "        [0.5345, 0.4655],\n",
       "        [0.8979, 0.1021],\n",
       "        [0.0329, 0.9671],\n",
       "        [0.3545, 0.6455],\n",
       "        [0.7311, 0.2689],\n",
       "        [0.7049, 0.2951],\n",
       "        [0.1555, 0.8445],\n",
       "        [0.1909, 0.8091],\n",
       "        [0.8085, 0.1915],\n",
       "        [0.3187, 0.6813],\n",
       "        [0.9070, 0.0930],\n",
       "        [0.8863, 0.1137],\n",
       "        [0.9348, 0.0652],\n",
       "        [0.0621, 0.9379],\n",
       "        [0.8956, 0.1044],\n",
       "        [0.8999, 0.1001],\n",
       "        [0.8976, 0.1024],\n",
       "        [0.2864, 0.7136],\n",
       "        [0.1265, 0.8735],\n",
       "        [0.0653, 0.9347],\n",
       "        [0.2507, 0.7493],\n",
       "        [0.8439, 0.1561],\n",
       "        [0.4263, 0.5737],\n",
       "        [0.2348, 0.7652],\n",
       "        [0.3015, 0.6985],\n",
       "        [0.9009, 0.0991],\n",
       "        [0.4779, 0.5221],\n",
       "        [0.1490, 0.8510],\n",
       "        [0.0517, 0.9483],\n",
       "        [0.4200, 0.5800],\n",
       "        [0.9195, 0.0805],\n",
       "        [0.1117, 0.8883],\n",
       "        [0.8974, 0.1026],\n",
       "        [0.3015, 0.6985],\n",
       "        [0.1072, 0.8928],\n",
       "        [0.6552, 0.3448],\n",
       "        [0.8782, 0.1218],\n",
       "        [0.9201, 0.0799],\n",
       "        [0.8750, 0.1250],\n",
       "        [0.8893, 0.1107],\n",
       "        [0.3211, 0.6789],\n",
       "        [0.4080, 0.5920],\n",
       "        [0.4220, 0.5780],\n",
       "        [0.0536, 0.9464],\n",
       "        [0.4014, 0.5986],\n",
       "        [0.9205, 0.0795],\n",
       "        [0.0442, 0.9558],\n",
       "        [0.9195, 0.0805],\n",
       "        [0.6124, 0.3876],\n",
       "        [0.9036, 0.0964],\n",
       "        [0.1107, 0.8893],\n",
       "        [0.9044, 0.0956],\n",
       "        [0.4222, 0.5778],\n",
       "        [0.9051, 0.0949],\n",
       "        [0.0430, 0.9570],\n",
       "        [0.9059, 0.0941],\n",
       "        [0.8979, 0.1021],\n",
       "        [0.9039, 0.0961],\n",
       "        [0.1332, 0.8668],\n",
       "        [0.8598, 0.1402],\n",
       "        [0.8662, 0.1338],\n",
       "        [0.8979, 0.1021],\n",
       "        [0.9172, 0.0828],\n",
       "        [0.7965, 0.2035],\n",
       "        [0.8425, 0.1575],\n",
       "        [0.2706, 0.7294],\n",
       "        [0.0476, 0.9524],\n",
       "        [0.2597, 0.7403],\n",
       "        [0.1007, 0.8993],\n",
       "        [0.8094, 0.1906],\n",
       "        [0.8873, 0.1127],\n",
       "        [0.2352, 0.7648],\n",
       "        [0.4644, 0.5356],\n",
       "        [0.0650, 0.9350],\n",
       "        [0.0715, 0.9285],\n",
       "        [0.9232, 0.0768],\n",
       "        [0.0371, 0.9629],\n",
       "        [0.9059, 0.0941],\n",
       "        [0.8979, 0.1021],\n",
       "        [0.3471, 0.6529],\n",
       "        [0.9020, 0.0980],\n",
       "        [0.3239, 0.6761],\n",
       "        [0.9196, 0.0804],\n",
       "        [0.9026, 0.0974],\n",
       "        [0.9049, 0.0951],\n",
       "        [0.4193, 0.5807],\n",
       "        [0.3489, 0.6511],\n",
       "        [0.9104, 0.0896],\n",
       "        [0.9210, 0.0790],\n",
       "        [0.9032, 0.0968],\n",
       "        [0.7956, 0.2044],\n",
       "        [0.8903, 0.1097],\n",
       "        [0.4391, 0.5609],\n",
       "        [0.9505, 0.0495],\n",
       "        [0.5784, 0.4216],\n",
       "        [0.0908, 0.9092],\n",
       "        [0.7851, 0.2149],\n",
       "        [0.8658, 0.1342],\n",
       "        [0.8620, 0.1380],\n",
       "        [0.9329, 0.0671],\n",
       "        [0.2721, 0.7279],\n",
       "        [0.9012, 0.0988],\n",
       "        [0.8597, 0.1403],\n",
       "        [0.8023, 0.1977],\n",
       "        [0.0259, 0.9741],\n",
       "        [0.8795, 0.1205],\n",
       "        [0.9622, 0.0378],\n",
       "        [0.5129, 0.4871],\n",
       "        [0.5779, 0.4221],\n",
       "        [0.9040, 0.0960],\n",
       "        [0.0581, 0.9419],\n",
       "        [0.4410, 0.5590],\n",
       "        [0.3372, 0.6628],\n",
       "        [0.4664, 0.5336],\n",
       "        [0.2710, 0.7290],\n",
       "        [0.1611, 0.8389],\n",
       "        [0.1091, 0.8909],\n",
       "        [0.9233, 0.0767],\n",
       "        [0.8582, 0.1418],\n",
       "        [0.4536, 0.5464],\n",
       "        [0.5312, 0.4688],\n",
       "        [0.9472, 0.0528],\n",
       "        [0.0391, 0.9609],\n",
       "        [0.4247, 0.5753],\n",
       "        [0.9213, 0.0787],\n",
       "        [0.7985, 0.2015],\n",
       "        [0.9062, 0.0938],\n",
       "        [0.8830, 0.1170],\n",
       "        [0.9589, 0.0411],\n",
       "        [0.0907, 0.9093],\n",
       "        [0.0928, 0.9072],\n",
       "        [0.6099, 0.3901],\n",
       "        [0.1100, 0.8900],\n",
       "        [0.0674, 0.9326],\n",
       "        [0.8974, 0.1026],\n",
       "        [0.3078, 0.6922],\n",
       "        [0.0309, 0.9691],\n",
       "        [0.8979, 0.1021],\n",
       "        [0.0248, 0.9752],\n",
       "        [0.9084, 0.0916],\n",
       "        [0.1521, 0.8479],\n",
       "        [0.9120, 0.0880],\n",
       "        [0.7291, 0.2709],\n",
       "        [0.9156, 0.0844],\n",
       "        [0.9149, 0.0851],\n",
       "        [0.8605, 0.1395],\n",
       "        [0.3084, 0.6916],\n",
       "        [0.9145, 0.0855],\n",
       "        [0.0516, 0.9484],\n",
       "        [0.9056, 0.0944],\n",
       "        [0.0451, 0.9549],\n",
       "        [0.4106, 0.5894],\n",
       "        [0.8892, 0.1108],\n",
       "        [0.2427, 0.7573],\n",
       "        [0.2411, 0.7589],\n",
       "        [0.1404, 0.8596],\n",
       "        [0.3081, 0.6919],\n",
       "        [0.0837, 0.9163],\n",
       "        [0.8942, 0.1058],\n",
       "        [0.7205, 0.2795],\n",
       "        [0.3759, 0.6241],\n",
       "        [0.8917, 0.1083],\n",
       "        [0.0471, 0.9529],\n",
       "        [0.9033, 0.0967],\n",
       "        [0.8584, 0.1416],\n",
       "        [0.9232, 0.0768],\n",
       "        [0.8301, 0.1699],\n",
       "        [0.1827, 0.8173],\n",
       "        [0.5350, 0.4650],\n",
       "        [0.6626, 0.3374],\n",
       "        [0.2697, 0.7303],\n",
       "        [0.8683, 0.1317],\n",
       "        [0.0866, 0.9134],\n",
       "        [0.9195, 0.0805],\n",
       "        [0.1009, 0.8991],\n",
       "        [0.9005, 0.0995],\n",
       "        [0.0494, 0.9506],\n",
       "        [0.9012, 0.0988],\n",
       "        [0.0561, 0.9439],\n",
       "        [0.1577, 0.8423],\n",
       "        [0.9027, 0.0973],\n",
       "        [0.2709, 0.7291],\n",
       "        [0.8935, 0.1065],\n",
       "        [0.9081, 0.0919],\n",
       "        [0.8379, 0.1621],\n",
       "        [0.0909, 0.9091],\n",
       "        [0.9095, 0.0905],\n",
       "        [0.8974, 0.1026],\n",
       "        [0.5029, 0.4971],\n",
       "        [0.9005, 0.0995],\n",
       "        [0.4372, 0.5628],\n",
       "        [0.7667, 0.2333],\n",
       "        [0.1204, 0.8796],\n",
       "        [0.0333, 0.9667],\n",
       "        [0.0581, 0.9419],\n",
       "        [0.1203, 0.8797],\n",
       "        [0.3953, 0.6047],\n",
       "        [0.9201, 0.0799],\n",
       "        [0.4642, 0.5358],\n",
       "        [0.4113, 0.5887],\n",
       "        [0.0642, 0.9358],\n",
       "        [0.9428, 0.0572],\n",
       "        [0.0650, 0.9350],\n",
       "        [0.2491, 0.7509],\n",
       "        [0.0684, 0.9316],\n",
       "        [0.9003, 0.0997],\n",
       "        [0.5005, 0.4995],\n",
       "        [0.8991, 0.1009],\n",
       "        [0.9017, 0.0983],\n",
       "        [0.9213, 0.0787],\n",
       "        [0.8979, 0.1021],\n",
       "        [0.9038, 0.0962],\n",
       "        [0.0860, 0.9140],\n",
       "        [0.9013, 0.0987],\n",
       "        [0.9177, 0.0823],\n",
       "        [0.9010, 0.0990],\n",
       "        [0.0843, 0.9157],\n",
       "        [0.3663, 0.6337],\n",
       "        [0.7496, 0.2504],\n",
       "        [0.9201, 0.0799],\n",
       "        [0.9486, 0.0514],\n",
       "        [0.9213, 0.0787],\n",
       "        [0.4105, 0.5895],\n",
       "        [0.8937, 0.1063],\n",
       "        [0.4929, 0.5071],\n",
       "        [0.8979, 0.1021],\n",
       "        [0.0300, 0.9700],\n",
       "        [0.3196, 0.6804],\n",
       "        [0.8830, 0.1170],\n",
       "        [0.1208, 0.8792],\n",
       "        [0.9002, 0.0998],\n",
       "        [0.9226, 0.0774],\n",
       "        [0.9124, 0.0876],\n",
       "        [0.8858, 0.1142],\n",
       "        [0.4336, 0.5664],\n",
       "        [0.3431, 0.6569],\n",
       "        [0.2709, 0.7291],\n",
       "        [0.2744, 0.7256],\n",
       "        [0.2808, 0.7192],\n",
       "        [0.9102, 0.0898],\n",
       "        [0.9224, 0.0776],\n",
       "        [0.7321, 0.2679],\n",
       "        [0.8317, 0.1683],\n",
       "        [0.9195, 0.0805],\n",
       "        [0.8465, 0.1535],\n",
       "        [0.3490, 0.6510],\n",
       "        [0.8830, 0.1170],\n",
       "        [0.7103, 0.2897],\n",
       "        [0.9029, 0.0971],\n",
       "        [0.9036, 0.0964],\n",
       "        [0.0433, 0.9567],\n",
       "        [0.8847, 0.1153],\n",
       "        [0.8053, 0.1947],\n",
       "        [0.9046, 0.0954],\n",
       "        [0.9055, 0.0945],\n",
       "        [0.1663, 0.8337],\n",
       "        [0.9511, 0.0489],\n",
       "        [0.9009, 0.0991],\n",
       "        [0.2709, 0.7291],\n",
       "        [0.1608, 0.8392],\n",
       "        [0.7129, 0.2871],\n",
       "        [0.1651, 0.8349],\n",
       "        [0.8102, 0.1898],\n",
       "        [0.4700, 0.5300],\n",
       "        [0.8953, 0.1047],\n",
       "        [0.7729, 0.2271],\n",
       "        [0.9212, 0.0788],\n",
       "        [0.3898, 0.6102],\n",
       "        [0.0313, 0.9687],\n",
       "        [0.2439, 0.7561],\n",
       "        [0.5656, 0.4344],\n",
       "        [0.8719, 0.1281],\n",
       "        [0.9040, 0.0960],\n",
       "        [0.9187, 0.0813],\n",
       "        [0.9039, 0.0961],\n",
       "        [0.7040, 0.2960],\n",
       "        [0.8447, 0.1553],\n",
       "        [0.8465, 0.1535],\n",
       "        [0.0800, 0.9200],\n",
       "        [0.9050, 0.0950],\n",
       "        [0.1415, 0.8585],\n",
       "        [0.6702, 0.3298],\n",
       "        [0.9086, 0.0914],\n",
       "        [0.8740, 0.1260],\n",
       "        [0.2221, 0.7779],\n",
       "        [0.4818, 0.5182],\n",
       "        [0.8830, 0.1170],\n",
       "        [0.1460, 0.8540],\n",
       "        [0.9039, 0.0961],\n",
       "        [0.6345, 0.3655],\n",
       "        [0.9011, 0.0989],\n",
       "        [0.9473, 0.0527],\n",
       "        [0.8364, 0.1636],\n",
       "        [0.1915, 0.8085],\n",
       "        [0.8669, 0.1331],\n",
       "        [0.9060, 0.0940],\n",
       "        [0.9079, 0.0921],\n",
       "        [0.0309, 0.9691],\n",
       "        [0.4511, 0.5489],\n",
       "        [0.3947, 0.6053],\n",
       "        [0.8903, 0.1097],\n",
       "        [0.2412, 0.7588],\n",
       "        [0.8836, 0.1164],\n",
       "        [0.1183, 0.8817],\n",
       "        [0.0304, 0.9696],\n",
       "        [0.8942, 0.1058],\n",
       "        [0.8301, 0.1699],\n",
       "        [0.9496, 0.0504],\n",
       "        [0.2982, 0.7018],\n",
       "        [0.4340, 0.5660],\n",
       "        [0.1020, 0.8980],\n",
       "        [0.9201, 0.0799],\n",
       "        [0.8979, 0.1021],\n",
       "        [0.4014, 0.5986],\n",
       "        [0.7656, 0.2344],\n",
       "        [0.0339, 0.9661],\n",
       "        [0.0702, 0.9298],\n",
       "        [0.9019, 0.0981],\n",
       "        [0.0281, 0.9719],\n",
       "        [0.5216, 0.4784],\n",
       "        [0.8926, 0.1074],\n",
       "        [0.3430, 0.6570],\n",
       "        [0.0672, 0.9328],\n",
       "        [0.7329, 0.2671],\n",
       "        [0.8984, 0.1016],\n",
       "        [0.0403, 0.9597],\n",
       "        [0.9359, 0.0641],\n",
       "        [0.9234, 0.0766],\n",
       "        [0.1008, 0.8992],\n",
       "        [0.1012, 0.8988],\n",
       "        [0.4718, 0.5282],\n",
       "        [0.8784, 0.1216],\n",
       "        [0.7579, 0.2421],\n",
       "        [0.5506, 0.4494],\n",
       "        [0.8979, 0.1021],\n",
       "        [0.8788, 0.1212],\n",
       "        [0.2030, 0.7970],\n",
       "        [0.3258, 0.6742],\n",
       "        [0.9078, 0.0922],\n",
       "        [0.2283, 0.7717],\n",
       "        [0.9034, 0.0966],\n",
       "        [0.9379, 0.0621],\n",
       "        [0.8666, 0.1334],\n",
       "        [0.3121, 0.6879],\n",
       "        [0.6455, 0.3545],\n",
       "        [0.0424, 0.9576],\n",
       "        [0.2025, 0.7975],\n",
       "        [0.9344, 0.0656],\n",
       "        [0.9467, 0.0533],\n",
       "        [0.0449, 0.9551],\n",
       "        [0.8782, 0.1218],\n",
       "        [0.0347, 0.9653],\n",
       "        [0.9020, 0.0980],\n",
       "        [0.8849, 0.1151],\n",
       "        [0.0547, 0.9453],\n",
       "        [0.9267, 0.0733],\n",
       "        [0.0728, 0.9272],\n",
       "        [0.7979, 0.2021],\n",
       "        [0.3197, 0.6803],\n",
       "        [0.1557, 0.8443],\n",
       "        [0.9056, 0.0944],\n",
       "        [0.6155, 0.3845],\n",
       "        [0.2711, 0.7289],\n",
       "        [0.4955, 0.5045],\n",
       "        [0.2709, 0.7291],\n",
       "        [0.0331, 0.9669],\n",
       "        [0.4731, 0.5269],\n",
       "        [0.9195, 0.0805],\n",
       "        [0.0638, 0.9362],\n",
       "        [0.9149, 0.0851],\n",
       "        [0.9195, 0.0805],\n",
       "        [0.2576, 0.7424]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ens_preds = torch.stack(learns).mean(0)\n",
    "ens_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db173320-7f47-4d3f-90be-99e8b2e57e6d",
   "metadata": {},
   "source": [
    "Use the same code as before to generate a submission file, which we can submit to Kaggle after the notebook is saved and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "38fca30a-6648-4ffa-91ae-09d97560c456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId,Survived\n",
      "892,0\n",
      "893,0\n",
      "894,0\n",
      "895,0\n",
      "896,1\n",
      "897,0\n",
      "898,1\n",
      "899,0\n",
      "900,1\n"
     ]
    }
   ],
   "source": [
    "test_df['Survived'] = (ens_preds[:,1]>0.5).int()\n",
    "subm_df = test_df[['PassengerId','Survived']]\n",
    "subm_df.to_csv('ens_subm.csv', index=False)\n",
    "!head ens_subm.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7942fd3-2797-4a68-ac41-aef3958baff3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
