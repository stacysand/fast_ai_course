{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2187ada-ad39-4a30-b22e-4e55357f07de",
   "metadata": {},
   "source": [
    "# 04_Lecture_NLP\n",
    "\n",
    "Fine-tuning a pre-trained NLP model for classification using Hugging Face Transformers library\n",
    "As a dataset we use U.S. Patent Phrase to Phrase Matching from Kaggel competition (https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching/data)\n",
    "\n",
    "**Comments:**  \n",
    "Hugging Face model hub: huggingface.co/models. Models are differ in training on different corpuses to solve different problems. By using search and a keyword, you can actually find a model that is trained on smth pretty similar to your task (trained on a same kind of documents). \n",
    "\n",
    "Article on how to split train data to valid dataset: https://www.fast.ai/2017/11/13/validation-sets/\n",
    "\n",
    "Kaggle has a second test set, which is yet another held-out dataset that's only used at the end of the competition to assess your predictions. That's called the \"private leaderboard\". Here's a great post about what can happen if you overfit to the public leaderboard: https://gregpark.io/blog/Kaggle-Psychopathy-Postmortem/\n",
    "\n",
    "The problem with metrics is a big problem for AI: https://www.fast.ai/2019/09/24/metrics/  \n",
    "In real life, outside of Kaggle, we don't really know what metrics to use... Here is more detail under the section \"Metrics and correlation\": https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners#Tokenization  \n",
    "When you work with new stat or evaluation metrics, play with graphs to understand how does it feel, what 0.34 looks like, or 0.68 or other values on that metric.\n",
    "\n",
    "Another Notebook how to do the same task on a high-level - Iterate Like a Grandmaster notebook (https://www.kaggle.com/code/jhoward/iterate-like-a-grandmaster/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1834b625-6d8d-44a7-bc4a-041101d6f5f4",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c578776-0a98-4755-a5c0-f2efa0d29429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # for work with dataframes\n",
    "from datasets import Dataset, DatasetDict  # for Hugging Face dataset (needed for tokenization) \n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer  # for tokenization\n",
    "from transformers import TrainingArguments, Trainer  # for training the model\n",
    "import numpy as np  # for operations with numbers and arrays\n",
    "import datasets  # for saving results as csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ede8ff-f8a5-487c-b8ff-29a832a0bb7f",
   "metadata": {},
   "source": [
    "# Step 1. Download the dataset\n",
    "1) Download the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bf0023-2fc1-450f-9a36-7b25f582cc27",
   "metadata": {},
   "source": [
    "The dataset was downloaded on PC from the web https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching/data and saved to the working directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbc2226-7fe7-49f6-bd10-634e9017c50b",
   "metadata": {},
   "source": [
    "2) Investigate the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fc19d64-e82f-405f-bc61-018f3945e05a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37d61fd2272659b1</td>\n",
       "      <td>abatement</td>\n",
       "      <td>abatement of pollution</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7b9652b17b68b7a4</td>\n",
       "      <td>abatement</td>\n",
       "      <td>act of abating</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36d72442aefd8232</td>\n",
       "      <td>abatement</td>\n",
       "      <td>active catalyst</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5296b0c19e1ce60e</td>\n",
       "      <td>abatement</td>\n",
       "      <td>eliminating process</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54c1e3b9184cb5b6</td>\n",
       "      <td>abatement</td>\n",
       "      <td>forest region</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36468</th>\n",
       "      <td>8e1386cbefd7f245</td>\n",
       "      <td>wood article</td>\n",
       "      <td>wooden article</td>\n",
       "      <td>B44</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36469</th>\n",
       "      <td>42d9e032d1cd3242</td>\n",
       "      <td>wood article</td>\n",
       "      <td>wooden box</td>\n",
       "      <td>B44</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36470</th>\n",
       "      <td>208654ccb9e14fa3</td>\n",
       "      <td>wood article</td>\n",
       "      <td>wooden handle</td>\n",
       "      <td>B44</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36471</th>\n",
       "      <td>756ec035e694722b</td>\n",
       "      <td>wood article</td>\n",
       "      <td>wooden material</td>\n",
       "      <td>B44</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36472</th>\n",
       "      <td>8d135da0b55b8c88</td>\n",
       "      <td>wood article</td>\n",
       "      <td>wooden substrate</td>\n",
       "      <td>B44</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36473 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id        anchor                  target context  score\n",
       "0      37d61fd2272659b1     abatement  abatement of pollution     A47   0.50\n",
       "1      7b9652b17b68b7a4     abatement          act of abating     A47   0.75\n",
       "2      36d72442aefd8232     abatement         active catalyst     A47   0.25\n",
       "3      5296b0c19e1ce60e     abatement     eliminating process     A47   0.50\n",
       "4      54c1e3b9184cb5b6     abatement           forest region     A47   0.00\n",
       "...                 ...           ...                     ...     ...    ...\n",
       "36468  8e1386cbefd7f245  wood article          wooden article     B44   1.00\n",
       "36469  42d9e032d1cd3242  wood article              wooden box     B44   0.50\n",
       "36470  208654ccb9e14fa3  wood article           wooden handle     B44   0.50\n",
       "36471  756ec035e694722b  wood article         wooden material     B44   0.75\n",
       "36472  8d135da0b55b8c88  wood article        wooden substrate     B44   0.50\n",
       "\n",
       "[36473 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('/Users/hela/Code/fast_ai/us-patent-phrase-to-phrase-matching/train.csv')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc792636-03d1-4cdb-ac81-4e9e55c88009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>36473</td>\n",
       "      <td>36473</td>\n",
       "      <td>36473</td>\n",
       "      <td>36473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>36473</td>\n",
       "      <td>733</td>\n",
       "      <td>29340</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>37d61fd2272659b1</td>\n",
       "      <td>component composite coating</td>\n",
       "      <td>composition</td>\n",
       "      <td>H01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>152</td>\n",
       "      <td>24</td>\n",
       "      <td>2186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                       anchor       target context\n",
       "count              36473                        36473        36473   36473\n",
       "unique             36473                          733        29340     106\n",
       "top     37d61fd2272659b1  component composite coating  composition     H01\n",
       "freq                   1                          152           24    2186"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe(include='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912e3127-a593-482a-b0f6-349d160c8e7f",
   "metadata": {},
   "source": [
    "# Step 2. Train_data: prepare input and output\n",
    "\n",
    "## 1) Input (tokenization & numeralization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed700429-208c-4711-b4e4-ae1c30234510",
   "metadata": {},
   "source": [
    "### 1) Code the input\n",
    "We can represent the input to the model as something like \"TEXT1: abatement; TEXT2: eliminating process\". (And the output will be a category of meaning similarity: \"Different; Similar; Identical\")  \n",
    "(we can refer to a column (also known as a series) either using regular python \"dotted\" notation, or access it like a dictionary. To define, use dict. To get the first few rows, use head():)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f67c6f0-2a67-4c38-a830-a0d80cb7ed8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    TEXT1: A47; TEXT2: abatement of pollution; ANC...\n",
       "1    TEXT1: A47; TEXT2: act of abating; ANC1: abate...\n",
       "2    TEXT1: A47; TEXT2: active catalyst; ANC1: abat...\n",
       "3    TEXT1: A47; TEXT2: eliminating process; ANC1: ...\n",
       "4    TEXT1: A47; TEXT2: forest region; ANC1: abatement\n",
       "Name: input, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['input'] = 'TEXT1: ' + train_df.context + '; TEXT2: ' + train_df.target + '; ANC1: ' + train_df.anchor\n",
    "train_df.input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b441e355-bfa1-423f-b016-47277e5d4832",
   "metadata": {},
   "source": [
    "### 2) Create Hugging Face dataset\n",
    "But we can't pass the texts directly into a model. A deep learning model expects numbers as inputs, not English sentences! So we need to do two things:\n",
    "\n",
    "1) Tokenization - split each text up into tokens\n",
    "2) Numericalization: Convert each token into a number\n",
    "\n",
    "These procedures will be performed through Hugging Face transformers ans Hugging Face datasets. So we need to turn our Pandas dataframe into Hugging Face \"datasets\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5001ec3-94a9-42e5-b914-33688bb45ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'anchor', 'target', 'context', 'score', 'input'],\n",
       "    num_rows: 36473\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = Dataset.from_pandas(train_df)\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95b1e6a-1025-4ee1-be02-3ed96b98dee2",
   "metadata": {},
   "source": [
    "### 3) Find a pre-trained model for tokenization\n",
    "The details about how tokenization and numeraliation are done actually depend on the particular model we use.   \n",
    "Hugging Face transformers is like timm, with a lot of pre-built models.  \n",
    "\n",
    "So first we'll need to pick a model. There are thousands of models available on Hugging Face..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "835c32aa-2783-45d9-b51c-fd02eb5e8f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'distilbert-base-uncased'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the one from the course had problems with issues with tokenizer loading so I tried another:\n",
    "# model_nm = 'distilbert/distilbert-base-uncased-finetuned-sst-2-english' # good, but num_labels=2 (trained for binary classification)\n",
    "model_nm = 'distilbert-base-uncased'\n",
    "model_nm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674f46e4-0216-419e-9e56-091fcfc77d97",
   "metadata": {},
   "source": [
    "### 4) Exctract AutoTokenizer from the model\n",
    "The reason why we pick the model is because we have to make sure we tokenize in the same way. (Thst's why I chose the model trained on English and on classification task. An ideall scenario is that the model is trained exactly for your task, of course. But I did not find potent models)  \n",
    "So to tell transformers that we want to tokenize the same way that the people that built the model did, we use:  \n",
    "AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c78b7b6d-b423-4b4d-83c7-962b54b1e19e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokz = AutoTokenizer.from_pretrained(model_nm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd54678-fb30-4fd6-8e85-0db0f74071a4",
   "metadata": {},
   "source": [
    "Check autotokenizer on any data:\n",
    "Here's an example of how the tokenizer splits a text into \"tokens\" (which are like words, but can be sub-word pieces). Everyone of these tokens is stored in the vocabulary that was created than the model was trained and has a number. So our tokens will be given a number, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae23487b-53b6-4f49-87bf-b594f39a6adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['g',\n",
       " \"'\",\n",
       " 'day',\n",
       " 'folks',\n",
       " ',',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'jeremy',\n",
       " 'from',\n",
       " 'fast',\n",
       " '.',\n",
       " 'ai',\n",
       " '!']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokz.tokenize(\"A platypus is an ornithorhynchus anatinus.\")  #uncommon words\n",
    "# or\n",
    "tokz.tokenize(\"G'day folks, I'm Jeremy from fast.ai!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5909567-c640-42cc-b31c-0bc81c67ad1a",
   "metadata": {},
   "source": [
    "### 6) Tokenize our input by autotokenizer\n",
    "Let's now perform tokenization of out input. This adds a new item to our dataset called input_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec04de2e-bbfe-4fd7-b373-ddaa2110ac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a fn\n",
    "def tok_fn(x):\n",
    "    return tokz(x['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f732feb6-85c6-480b-a3a5-b03458db5256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273abcdc01c644cdb382fd65a9ec7103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run fn quickly in parallel on every row in our dataset using 'map':\n",
    "tok_train_ds = train_ds.map(tok_fn, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1a8a21-054a-448a-a9b7-713b8b1b1caf",
   "metadata": {},
   "source": [
    "Check the result.  \n",
    "For instance, let's look at the input and correspoding IDs (token numbers) for the first row of our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ae026e3-d910-4f6a-8f44-17a681c6d280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT1: A47; TEXT2: abatement of pollution; ANC1: abatement\n",
      "[101, 3793, 2487, 1024, 1037, 22610, 1025, 3793, 2475, 1024, 19557, 18532, 4765, 1997, 10796, 1025, 2019, 2278, 2487, 1024, 19557, 18532, 4765, 102]\n"
     ]
    }
   ],
   "source": [
    "print(tok_train_ds['input'][0])\n",
    "print(tok_train_ds['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46dbf0d2-d825-4475-a9c9-44dbf5ce938a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1037"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokz.vocab['a']  # and we see this number in our tokens above, indeed, where A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90d0f8c-d93c-4634-ae64-ffb1712e0c2a",
   "metadata": {},
   "source": [
    "## 2) Output\n",
    "Finally, we need to prepare our labels. Transformers always assumes that your labels has the column name labels, but in our dataset it's currently \"score\". Therefore, we need to rename it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f2c718c-bf7b-4bef-84be-0be8c9e70490",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_train_ds = tok_train_ds.rename_columns({'score':'labels'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0121ff02-ab96-4cc7-8a7d-2dab5654891a",
   "metadata": {},
   "source": [
    "# Step 3. Prepare test_data & valid_data\n",
    "## 1) Valid_data\n",
    "Transformers uses a DatasetDict for holding our training and validation sets. To create one that contains 25% of our data for the validation set, and 75% for the training set, use train_test_split.  \n",
    "As you see above, the validation set here is called test and not validate, so be careful!!!  \n",
    "Also, in practice, a random split like we've used here might not be a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d488b75e-0824-4c21-b3b6-9f21e84813b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'anchor', 'target', 'context', 'labels', 'input', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 27354\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'anchor', 'target', 'context', 'labels', 'input', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 9119\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dds = tok_train_ds.train_test_split(0.25, seed=42)\n",
    "dds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10533dc7-bda7-4dde-9b7b-f7a649eeb3dc",
   "metadata": {},
   "source": [
    "## 2) Test_data\n",
    "Our directory contained another file - test data. We'll use 'eval_data' as our name for the test set, to avoid confusion with the test dataset for validation data that was created above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a8403fa-534d-4358-a612-59d02658a19c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>36</td>\n",
       "      <td>34</td>\n",
       "      <td>36</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>4112d61851461f60</td>\n",
       "      <td>el display</td>\n",
       "      <td>inorganic photoconductor drum</td>\n",
       "      <td>G02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id      anchor                         target context\n",
       "count                 36          36                             36      36\n",
       "unique                36          34                             36      29\n",
       "top     4112d61851461f60  el display  inorganic photoconductor drum     G02\n",
       "freq                   1           2                              1       3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df = pd.read_csv('/Users/hela/Code/fast_ai/us-patent-phrase-to-phrase-matching/test.csv')\n",
    "eval_df\n",
    "eval_df.describe(include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0684c1f0-fe74-48bf-88bc-2fd3409a81b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc245a27e2ab423893481781ed8c096d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create an input for evaluaiton data:\n",
    "eval_df['input'] = 'TEXT1: ' + eval_df.context + '; TEXT2: ' + eval_df.target + '; ANC1: ' + eval_df.anchor\n",
    "# turn into a dataset and perform tokenization:\n",
    "tok_eval_ds = Dataset.from_pandas(eval_df).map(tok_fn, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e47695c-9a82-4aae-b676-c5558456f44a",
   "metadata": {},
   "source": [
    "Check the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9cdd8ef-3f99-4e81-b75c-31beef7da9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT1: G02; TEXT2: inorganic photoconductor drum; ANC1: opc drum\n",
      "[101, 3793, 2487, 1024, 1043, 2692, 2475, 1025, 3793, 2475, 1024, 28256, 6302, 8663, 8566, 16761, 6943, 1025, 2019, 2278, 2487, 1024, 6728, 2278, 6943, 102]\n"
     ]
    }
   ],
   "source": [
    "print(tok_eval_ds['input'][0])\n",
    "print(tok_eval_ds['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f642823d-5c98-42ff-b4c9-d43a0fb1c7ff",
   "metadata": {},
   "source": [
    "## 3) Evaluation metrics: Pearson corr\n",
    "For this particular task, Kaggel competition says that the evaluation metric is Pearson correlation coefficient between the predicted and actual output (which is, similarity of TEXT1 and TEXT2). Remember that with Pearson, even several outliers can significantly influence your results, so they need to be deleted. Same with NN - even a couple of rows really badly wrong can destroy everything. So you want to be sure that you perform a good job on eny row.\n",
    "\n",
    "But in real life, outside of Kaggle, we don't really know what metrics to use... Here is more detail under the section \"Metrics and correlation\": https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners#Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c0af24-65f1-4a1d-8296-5537ece5e934",
   "metadata": {},
   "source": [
    "We need the report of the correlation after each epoch.  \n",
    "Transformers expects you to return metrics as a dictionary, because it will use the keys of the dictionary to label each metric. So let's create a fn that count correlation and returns it as a dictionary with a label \"pearson\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5bc39305-9a34-49f0-9039-f2cc5adc0340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old from the lecture:\n",
    "def corr_d(eval_pred):\n",
    "    return {'pearson': pearsonr(*eval_pred)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53157713-858b-44dd-b08b-2fb3ff80341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrected by me, so it works:\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def corr_d(eval_pred):\n",
    "    predictions, label_ids = eval_pred\n",
    "    print(\"Predictions shape:\", predictions.shape)\n",
    "    print(\"Label_ids shape:\", label_ids.shape)\n",
    "    # Flatten arrays to ensure 1D\n",
    "    predictions = np.array(predictions).flatten()\n",
    "    label_ids = np.array(label_ids).flatten()\n",
    "    print(\"Flattened predictions shape:\", predictions.shape)\n",
    "    print(\"Flattened label_ids shape:\", label_ids.shape)\n",
    "    if len(predictions) != len(label_ids):\n",
    "        raise ValueError(f\"Mismatch: predictions ({len(predictions)}) and labels ({len(label_ids)}) have different lengths\")\n",
    "    if len(predictions) < 2:\n",
    "        return {'pearson': 0.0}  # Handle edge case\n",
    "    return {'pearson': pearsonr(predictions, label_ids)[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b7e0d4-7a4b-43ca-a2d1-a9d74966a29d",
   "metadata": {},
   "source": [
    "# Step 4. Train the model\n",
    "We will train the model in Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dced751-1daa-437f-bf04-2afb4926e5ef",
   "metadata": {},
   "source": [
    "## 1) Define hyperparameters\n",
    "Learning rate = fastai provides a learning rate finder to help you figure this out, but Transformers doesn't, so you'll just have to use trial and error. The idea is to find the largest value you can, but which doesn't result in training failing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e791a8e3-c18a-4f5b-a231-aea3cf2340c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 128  # batch size: find the one that fits your GPU\n",
    "epochs = 4  # to run quickly\n",
    "lr = 8e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dd47c2-52e6-4732-82bf-86cd5ac4172a",
   "metadata": {},
   "source": [
    "## 2) Set up arguments for Transformers\n",
    "Transformers uses the TrainingArguments class to set up arguments. Don't worry too much about the values we're using here -- they should generally work fine in most cases. It's just the 3 parameters above that you may need to change for different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6d13edc-ac81-433e-8fe0-e46c21724c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments('outputs', \n",
    "                         learning_rate=lr, \n",
    "                         warmup_ratio=0.1, \n",
    "                         lr_scheduler_type='cosine', \n",
    "                         fp16=False,  # Disable FP16 to use fp32\n",
    "                         bf16=False,  # Disable BF16 to use fp32\n",
    "                         eval_strategy=\"epoch\", \n",
    "                         per_device_train_batch_size=bs, \n",
    "                         per_device_eval_batch_size=bs*2,\n",
    "                         num_train_epochs=epochs, \n",
    "                         weight_decay=0.01, \n",
    "                         report_to='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb2f08a-0899-46f4-8c54-d9314240e364",
   "metadata": {},
   "source": [
    "## 3) Create a model and a Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4d0e46f-239d-49d9-9ea4-307284eaa847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "116cf3aa-1843-4b58-8511-b557d60c290b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, \n",
    "                  args,\n",
    "                  train_dataset=dds['train'],\n",
    "                  eval_dataset=dds['test'],\n",
    "                  processing_class=tokz, \n",
    "                  compute_metrics=corr_d)\n",
    "# I used processing_class instead of tokenizer because it was an update in the Trainer() class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7352e2de-4011-4519-afd8-f63ca957332f",
   "metadata": {},
   "source": [
    "## 4) Train our model\n",
    "The key thing to look at is the \"Pearson\" value in table above. As you see, it's increasing, and is already above 0.8. That's great news! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27eef114-464b-424b-9a14-f0517b62c075",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hela/Code/fast_ai/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='856' max='856' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [856/856 12:42, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.034373</td>\n",
       "      <td>0.730294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.026375</td>\n",
       "      <td>0.776283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.034600</td>\n",
       "      <td>0.024545</td>\n",
       "      <td>0.793398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.034600</td>\n",
       "      <td>0.025271</td>\n",
       "      <td>0.793651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (9119, 1)\n",
      "Label_ids shape: (9119,)\n",
      "Flattened predictions shape: (9119,)\n",
      "Flattened label_ids shape: (9119,)\n",
      "Predictions shape: (9119, 1)\n",
      "Label_ids shape: (9119,)\n",
      "Flattened predictions shape: (9119,)\n",
      "Flattened label_ids shape: (9119,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hela/Code/fast_ai/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (9119, 1)\n",
      "Label_ids shape: (9119,)\n",
      "Flattened predictions shape: (9119,)\n",
      "Flattened label_ids shape: (9119,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hela/Code/fast_ai/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (9119, 1)\n",
      "Label_ids shape: (9119,)\n",
      "Flattened predictions shape: (9119,)\n",
      "Flattened label_ids shape: (9119,)\n"
     ]
    }
   ],
   "source": [
    "trainer.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644cf262-48b1-4a34-a4a2-8772a32e2013",
   "metadata": {},
   "source": [
    "# Step 5. Check the results\n",
    "## 1) Run model on test data\n",
    "Let's get some predictions on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0503bb63-146a-42d3-9d36-1a104774829e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.46],\n",
       "       [ 0.54],\n",
       "       [ 0.47],\n",
       "       [ 0.29],\n",
       "       [-0.01],\n",
       "       [ 0.54],\n",
       "       [ 0.52],\n",
       "       [-0.03],\n",
       "       [ 0.21],\n",
       "       [ 0.98],\n",
       "       [ 0.19],\n",
       "       [ 0.34],\n",
       "       [ 0.76],\n",
       "       [ 0.76],\n",
       "       [ 0.73],\n",
       "       [ 0.42],\n",
       "       [ 0.28],\n",
       "       [ 0.02],\n",
       "       [ 0.56],\n",
       "       [ 0.35],\n",
       "       [ 0.44],\n",
       "       [ 0.17],\n",
       "       [ 0.09],\n",
       "       [ 0.24],\n",
       "       [ 0.42],\n",
       "       [ 0.  ],\n",
       "       [-0.02],\n",
       "       [ 0.  ],\n",
       "       [-0.03],\n",
       "       [ 0.69],\n",
       "       [ 0.32],\n",
       "       [ 0.  ],\n",
       "       [ 0.73],\n",
       "       [ 0.51],\n",
       "       [ 0.32],\n",
       "       [ 0.27]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = trainer.predict(tok_eval_ds).predictions.astype(float)\n",
    "# Round to 2 decimal places for readability\n",
    "preds_rounded = np.round(preds, decimals=2)\n",
    "preds_rounded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b3a06f-d4c0-415b-a3d0-9ae28a8e112b",
   "metadata": {},
   "source": [
    "## 2) Fix out-of-bounds predictions\n",
    "Look out - some of our predictions are <0, or >1! This once again shows the value of remember to actually look at your data. Let's fix those out-of-bounds predictions... For now we will do it in a primitive way, just making values being <0 equal 0, and values >1 being equal 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad4691df-4031-4576-847d-65f119f19cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46],\n",
       "       [0.54],\n",
       "       [0.47],\n",
       "       [0.29],\n",
       "       [0.  ],\n",
       "       [0.54],\n",
       "       [0.52],\n",
       "       [0.  ],\n",
       "       [0.21],\n",
       "       [0.98],\n",
       "       [0.19],\n",
       "       [0.34],\n",
       "       [0.76],\n",
       "       [0.76],\n",
       "       [0.73],\n",
       "       [0.42],\n",
       "       [0.28],\n",
       "       [0.02],\n",
       "       [0.56],\n",
       "       [0.35],\n",
       "       [0.44],\n",
       "       [0.17],\n",
       "       [0.09],\n",
       "       [0.24],\n",
       "       [0.42],\n",
       "       [0.  ],\n",
       "       [0.  ],\n",
       "       [0.  ],\n",
       "       [0.  ],\n",
       "       [0.69],\n",
       "       [0.32],\n",
       "       [0.  ],\n",
       "       [0.73],\n",
       "       [0.51],\n",
       "       [0.32],\n",
       "       [0.27]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = np.clip(preds_rounded, 0, 1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95e14ef-70c7-4c87-908c-e9f6d6f2f9c7",
   "metadata": {},
   "source": [
    "# Step 6. Save the results as CSV\n",
    "OK, now we're ready to create our submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b48f2b0-07bd-478e-89f4-46ff0883af2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aaa40bd16594843a92f2c161311c2ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "859"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = datasets.Dataset.from_dict({\n",
    "    'id': tok_eval_ds['id'],\n",
    "    'score': preds\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
